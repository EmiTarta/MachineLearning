{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementación del Modelo Predictivo en un Pipeline Automático\n",
    "\n",
    "En este notebook, se desarrolla un pipeline completo y automatizado para la predicción de salarios en el sector tecnológico. Este pipeline integra la transformación de datos, el escalado, y el modelo de predicción en un único flujo de trabajo, optimizando el rendimiento y garantizando la reproducibilidad. Los pasos clave implementados son:\n",
    "\n",
    "#### 1) Preprocesamiento Personalizado:\n",
    "- Se diseñó una clase específica, CustomPreprocessor, que realiza múltiples tareas de transformación:\n",
    "    - Mapeos Ordinales y Categóricos: Traduce valores textuales (como nivel educativo o frecuencia de actividades) a representaciones numéricas utilizando diccionarios definidos previamente.\n",
    "    - Codificación One-Hot y MultiLabel: Convierte columnas categóricas y con múltiples etiquetas (como tecnologías y lenguajes utilizados) en representaciones binarizadas para su uso en modelos de machine learning.\n",
    "    - Target Encoding: Asigna un valor promedio basado en la variable objetivo (`CompTotal`) para ciertas categorías seleccionadas, incorporando información sobre su relación directa con la variable objetivo.\n",
    "    - Tratamiento de Valores Faltantes: Imputa valores ausentes según la naturaleza de cada columna (mediana, moda o valores predefinidos).\n",
    "#### 2) Escalado y Modelo:\n",
    "- Se utiliza un MinMaxScaler para normalizar las características numéricas, asegurando que todas las variables estén dentro del mismo rango.\n",
    "- El modelo principal es un VotingRegressor que combina tres algoritmos base: Random Forest, Gradient Boosting, y XGBoost. Este modelo se entrena previamente y se integra directamente al pipeline\n",
    "#### 3) Pipeline Final:\n",
    "- El pipeline integra el preprocesador personalizado (`CustomPreprocessor`), el escalador, y el modelo de predicción en un único flujo de trabajo. Este diseño asegura que los datos brutos puedan ser transformados y utilizados para predicciones de manera consistente y sin pasos manuales intermedios.\n",
    "#### 4) Manejo de Datos:\n",
    "- Se combinan los datasets de las encuestas de Stack Overflow de los años 2023 y 2024, seleccionando las columnas más relevantes y filtrando los datos según un rango razonable de salarios (18,000 a 55,000 euros).\n",
    "- La variable objetivo (CompTotal) se transforma logarítmicamente para manejar su distribución asimétrica y mejorar el rendimiento del modelo.\n",
    "#### 5) Exportación del Pipeline:\n",
    "- Una vez entrenado, el pipeline completo se guarda en un archivo mediante joblib, permitiendo su reutilización para nuevas predicciones sin necesidad de repetir el proceso de preprocesamiento y entrenamiento.\n",
    "\n",
    "### Propósito de la Clase CustomPreprocessor\n",
    "La clase `CustomPreprocessor` es un componente clave del pipeline que permite procesar datos brutos y prepararlos para el modelo. Esta clase ofrece las siguientes ventajas:\n",
    "- Automatización del Preprocesamiento: Integra múltiples técnicas de transformación en una sola clase, reduciendo la necesidad de pasos manuales.\n",
    "- Adaptabilidad: Permite manejar columnas categóricas, ordinales, y multilabel con facilidad, incorporando target encoding y codificación binarizada según sea necesario.\n",
    "- Escalabilidad: Diseñada para manejar grandes conjuntos de datos con múltiples columnas de diferentes tipos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline guardado exitosamente en: ..\\Pickles\\Pipeline.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "import xgboost as xgb\n",
    "import pickle\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "import joblib\n",
    "import os\n",
    "from custom_preprocessor import CustomPreprocessor  # Importar clase personalizada para preprocesamiento\n",
    "\n",
    "# Mapeo de niveles educativos a valores ordinales\n",
    "edlevel_mapping = {\n",
    "    'Master’s degree (M.A., M.S., M.Eng., MBA, etc.)': 5,\n",
    "    'Bachelor’s degree (B.A., B.S., B.Eng., etc.)': 4,\n",
    "    'Professional degree (JD, MD, Ph.D, Ed.D, etc.)': 6,\n",
    "    'Some college/university study without earning a degree': 2,\n",
    "    'Secondary school (e.g. American high school)': 1,\n",
    "    'Associate degree (A.A., A.S., etc.)': 3,\n",
    "    'Something else': -1,\n",
    "    'Primary/elementary school': 0\n",
    "}\n",
    "\n",
    "# Mapeo de industrias a categorías generalizadas\n",
    "industry_mapping = {\n",
    "    'Information Services, IT, Software Development, or other Technology': 'Tecnología y Servicios Digitales',\n",
    "    'Healthcare': 'Salud y Educación',\n",
    "    'Retail and Consumer Services': 'Otros Servicios',\n",
    "    'Legal Services': 'Otros Servicios',\n",
    "    'Higher Education': 'Salud y Educación',\n",
    "    'Financial Services': 'Servicios Financieros',\n",
    "    'Manufacturing': 'Industria y Energía',\n",
    "    'Insurance': 'Servicios Financieros',\n",
    "    'Oil & Gas': 'Industria y Energía'\n",
    "}\n",
    "\n",
    "# Mapeo de frecuencia a valores ordinales\n",
    "frequency_mapping = {\n",
    "    '10+ times a week': 4, \n",
    "    '6-10 times a week': 3, \n",
    "    '3-5 times a week': 2,\n",
    "    '1-2 times a week': 1, \n",
    "    'Never': 0, \n",
    "    'Other': -1\n",
    "}\n",
    "\n",
    "# Columnas objetivo para encoding personalizado\n",
    "target_columns = ['LearnCodeOnline', 'DevType', 'LearnCode', 'CodingActivities']\n",
    "\n",
    "# Columnas multilabel para codificación binarizada\n",
    "multi_label_columns = ['DatabaseHaveWorkedWith', 'LanguageWantToWorkWith', 'LanguageHaveWorkedWith', 'ToolsTechHaveWorkedWith']\n",
    "\n",
    "# Inicialización del preprocesador personalizado\n",
    "preprocessor = CustomPreprocessor(target_columns, multi_label_columns, edlevel_mapping, industry_mapping, frequency_mapping)\n",
    "\n",
    "# Cargar el scaler y modelo previamente entrenados desde archivos\n",
    "ruta_pickles = os.path.join(\"..\", \"Pickles\")\n",
    "ruta_scaler = os.path.join(ruta_pickles, 'Scaler.pkl')\n",
    "ruta_modelo = os.path.join(ruta_pickles, 'MiModelo.pkl')\n",
    "\n",
    "scaler = joblib.load(ruta_scaler)  # Carga del escalador\n",
    "voting_regressor = joblib.load(ruta_modelo)  # Carga del modelo de votación\n",
    "\n",
    "# Definición del pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),  # Preprocesador personalizado\n",
    "    ('scaler', scaler),              # Escalador para normalización\n",
    "    ('model', voting_regressor)      # Modelo final\n",
    "])\n",
    "\n",
    "# Cargar datos de entrada desde archivos pickle\n",
    "with open('../Pickles/data_2023.pickle', 'rb') as archivo:   \n",
    "    df1 = pickle.load(archivo)\n",
    "with open('../Pickles/data_2024.pickle', 'rb') as archivo:\n",
    "    df2 = pickle.load(archivo)\n",
    "\n",
    "# Combinar datasets de años distintos\n",
    "df = pd.concat([df1, df2], ignore_index=True, join='inner')\n",
    "\n",
    "# Selección de columnas relevantes\n",
    "df = df[['YearsCodePro', 'LearnCodeOnline', 'DevType', 'LearnCode', 'CodingActivities', \n",
    "         'DatabaseHaveWorkedWith', 'YearsCode', 'LanguageWantToWorkWith', \n",
    "         'LanguageHaveWorkedWith', 'EdLevel', 'Employment', 'ToolsTechHaveWorkedWith', \n",
    "         'AISent', 'Industry', 'Frequency_2', 'Frequency_1', 'CompTotal']]\n",
    "\n",
    "# Filtrar datos según valores razonables en la variable objetivo (CompTotal)\n",
    "limite_inferior = 18000\n",
    "limite_superior = 55000\n",
    "df = df[(df['CompTotal'] >= limite_inferior) & (df['CompTotal'] <= limite_superior)]\n",
    "\n",
    "# Separar variable objetivo (y) y características (X)\n",
    "y = np.log1p(df['CompTotal'])  # Transformación logarítmica de la variable objetivo\n",
    "X = df.drop(columns=['CompTotal'])  # Eliminar CompTotal de las características\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar el pipeline completo con los datos de entrenamiento\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Define la ruta para guardar el pipeline entrenado\n",
    "ruta_pipeline = os.path.join(ruta_pickles, \"Pipeline.pkl\")\n",
    "os.makedirs(ruta_pickles, exist_ok=True)  # Crear carpeta si no existe\n",
    "\n",
    "# Guardar el pipeline en un archivo\n",
    "joblib.dump(pipeline, ruta_pipeline)\n",
    "\n",
    "# Confirmación de guardado exitoso\n",
    "print(f\"Pipeline guardado exitosamente en: {ruta_pipeline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicciones con nuevas muestras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook representa el cierre del proceso de desarrollo del modelo predictivo, ofreciendo una solución completamente funcional y automatizada para la predicción de salarios. Con el pipeline entrenado y exportado, el siguiente paso será probar su rendimiento con nuevos datos de entrada, validando su capacidad de generalización en escenarios reales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo una nueva muestra de 3 variables para probar predicciones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = pd.DataFrame({\n",
    "    'YearsCodePro': [3, 5, 10],\n",
    "    'LearnCodeOnline': ['How-to videos;Online Forum', 'How-to videos;Online Forum', 'Online Forum'],\n",
    "    'DevType': ['Developer, back-end', 'Developer, full-stack', 'Developer, mobile'],\n",
    "    'LearnCode': ['On the job training', 'On the job training;School (i.e., University, College, etc)', 'On the job training;School (i.e., University, College, etc)'],\n",
    "    'CodingActivities': ['Freelance/contract work', 'Hobby;Freelance/contract work', 'Hobby;Contribute to open-source projects'],\n",
    "    'DatabaseHaveWorkedWith': ['MySQL', 'PostgreSQL;MySQL', 'PostgreSQL;SQLite'],\n",
    "    'YearsCode': [5, 10, 15],\n",
    "    'LanguageWantToWorkWith': ['Python;JavaScript', 'Java;Python', 'C++;Python'],\n",
    "    'LanguageHaveWorkedWith': ['Python;JavaScript', 'Java;Python;SQL', 'C++;Python;SQL'],\n",
    "    'EdLevel': ['Master’s degree (M.A., M.S., M.Eng., MBA, etc.)', 'Bachelor’s degree (B.A., B.S., B.Eng., etc.)', 'Some college/university study without earning a degree'],\n",
    "    'Employment': ['Employed, full-time', 'Employed, full-time', 'Employed, full-time'],\n",
    "    'ToolsTechHaveWorkedWith': ['Docker', 'Docker;Kubernetes', 'Git'],\n",
    "    'AISent': ['Favorable', 'Unsure', 'Favorable'],\n",
    "    'Industry': ['Information Services, IT, Software Development, or other Technology', 'Financial Services', 'Manufacturing'],\n",
    "    'Frequency_2': ['3-5 times a week', '1-2 times a week', 'Never'],\n",
    "    'Frequency_1': ['1-2 times a week', 'Never', '6-10 times a week']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación transformo los datos nuevos, adaptando los nombres de las columnas y generando un DataFrame compatible con el pipeline, asegurando que los datos estén en el formato esperado para el modelo.\n",
    "1) Utilizo el preprocesador del pipeline (CustomPreprocessor) para aplicar las transformaciones definidas previamente. Estas transformaciones incluyen la codificación de variables categóricas, el mapeo de valores ordinales, la binarización de variables multilabel, entre otras, adaptando los datos a la estructura requerida por el modelo.\n",
    "\n",
    "2) Se genera una lista de los nombres de las columnas resultantes después del preprocesamiento. Esto es útil porque las transformaciones pueden cambiar los nombres de las columnas originales (por ejemplo, al crear columnas codificadas con one-hot encoding o binarización).\n",
    "\n",
    "3) Convierte los datos preprocesados en un nuevo DataFrame, asignando los nombres de las columnas generados en el paso anterior. Este DataFrame es estructurado y listo para ser utilizado como entrada al modelo de predicción.\n",
    "\n",
    "4) Relleno los Nan con 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformar los datos nuevos (new_data) con el preprocesador\n",
    "preprocessed_data = pipeline.named_steps['preprocessor'].transform(new_data)\n",
    "# Obtener los nombres de las columnas resultantes\n",
    "column_names = pipeline.named_steps['preprocessor'].get_feature_names_out(input_features=new_data.columns)\n",
    "# Crear un DataFrame con los datos preprocesados\n",
    "preprocessed_df = pd.DataFrame(preprocessed_data, columns=column_names)\n",
    "preprocessed_df = preprocessed_df.fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego utilizo el modelo final del pipeline para predecir el salario basado en los datos preprocesados contenidos en preprocessed_df. \n",
    "\n",
    "Este fragmento aplica el modelo de predicción, convierte los resultados de la escala logarítmica a la escala original, y los presenta en una serie con valores de salario estimados. Es el paso final para obtener predicciones utilizables del modelo entrenado.\n",
    "\n",
    "1) El modelo devuelve los valores de predicción en escala logarítmica porque el objetivo (CompTotal) se transformó con np.log1p durante el entrenamiento.\n",
    "\n",
    "2) Transformo los valores de predicción de vuelta a la escala original del salario utilizando la función np.expm1. Esto revierte la transformación logarítmica aplicada previamente, haciendo que los resultados sean interpretables como salarios en su unidad original.\n",
    "\n",
    "3) Muestro la serie resultante con las predicciones del salario en su escala original. Cada valor de esta serie corresponde a la predicción del salario para una fila en los datos de entrada new_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    22439.231636\n",
      "1    23256.987311\n",
      "2    22392.101585\n",
      "Name: Predicted Salary, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but RandomForestRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but GradientBoostingRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Realizar la predicción con las columnas finales\n",
    "predicted_salary_log = pipeline.named_steps['model'].predict(preprocessed_df)\n",
    "\n",
    "# Convertir de escala logarítmica a escala original\n",
    "predicted_salary = pd.Series(np.expm1(predicted_salary_log), name=\"Predicted Salary\")\n",
    "\n",
    "# Verificar resultados\n",
    "print(predicted_salary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el siguiente notebook haré un Streamlit para poder desplegar el modelo en una web. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proyecto_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
