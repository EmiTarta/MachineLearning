{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, cross_validate, cross_val_predict, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset con filtro de las features elegidas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del DataFrame original: (1934, 397)\n",
      "Tamaño del DataFrame filtrado: (1743, 397)\n"
     ]
    }
   ],
   "source": [
    "# Cargar el DataFrame desde el archivo\n",
    "with open('../Pickles/df_final.pickle', 'rb') as archivo:\n",
    "    df = pickle.load(archivo)\n",
    "# Establecer la opción para mostrar todas las columnas\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "nombres=df.drop(['CompTotal'], axis=1).columns\n",
    "\n",
    "limite_inferior = 18000\n",
    "limite_superior = 100000\n",
    "\n",
    "df_filtrado = df[(df['CompTotal'] >= limite_inferior) & (df['CompTotal'] <= limite_superior)]\n",
    "\n",
    "print(f\"Tamaño del DataFrame original: {df.shape}\")\n",
    "print(f\"Tamaño del DataFrame filtrado: {df_filtrado.shape}\")\n",
    "\n",
    "df = df_filtrado\n",
    "\n",
    "df['CompTotal'] = np.log1p(df['CompTotal'])\n",
    "y = df['CompTotal']\n",
    "df = df.drop(columns=['CompTotal'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df[['MainBranch','EdLevel','YearsCode','YearsCodePro','AISelect', 'AISent',  'Frequency_1',\n",
    "        'Frequency_2','Frequency_3', 'Age_Grouped_Adulto','Age_Grouped_Joven','Age_Grouped_Senior', \n",
    "        'is_full_time', 'is_part_time', 'is_independent', 'num_jobs', 'is_other_employment', \n",
    "        'CodingActivities_encoded', 'LearnCode_encoded', 'LearnCodeOnline_encoded', \n",
    "        'DevType_encoded', 'Android-based', 'Linux-based', 'Windows-based',\n",
    "        'Industry_Category_Industria y Energía','Industry_Category_Otros Servicios',\n",
    "        'Industry_Category_Salud y Educación','Industry_Category_Servicios Financieros',\n",
    "        'Industry_Category_Tecnología y Servicios Digitales', 'BuyNewTool_Ask a generative AI tool',\n",
    "        'BuyNewTool_Ask developers I know/work with','BuyNewTool_Other','BuyNewTool_Other (please specify):',\n",
    "        'BuyNewTool_Read ratings or reviews on third party sites like G2 Crowd',\n",
    "        'BuyNewTool_Research companies that have advertised on sites I visit',\n",
    "        'BuyNewTool_Research companies that have emailed me','BuyNewTool_Start a free trial',\n",
    "        'BuyNewTool_Visit developer communities like Stack Overflow', 'LanguageHaveWorkedWith_', \n",
    "        'LanguageHaveWorkedWith_Ada', 'LanguageHaveWorkedWith_Apex', 'LanguageHaveWorkedWith_Assembly',\n",
    "        'LanguageHaveWorkedWith_Bash/Shell (all shells)', 'LanguageHaveWorkedWith_C', \n",
    "        'LanguageHaveWorkedWith_C#','LanguageHaveWorkedWith_C++', 'LanguageHaveWorkedWith_Clojure', \n",
    "        'LanguageHaveWorkedWith_Cobol','LanguageHaveWorkedWith_Crystal','LanguageHaveWorkedWith_Dart',\n",
    "        'LanguageHaveWorkedWith_Delphi','LanguageHaveWorkedWith_Elixir','LanguageHaveWorkedWith_Erlang',\n",
    "        'LanguageHaveWorkedWith_F#','LanguageHaveWorkedWith_Fortran','LanguageHaveWorkedWith_GDScript',\n",
    "        'LanguageHaveWorkedWith_Go','LanguageHaveWorkedWith_Groovy','LanguageHaveWorkedWith_HTML/CSS',\n",
    "        'LanguageHaveWorkedWith_Haskell','LanguageHaveWorkedWith_Java','LanguageHaveWorkedWith_JavaScript',\n",
    "        'LanguageHaveWorkedWith_Julia','LanguageHaveWorkedWith_Kotlin','LanguageHaveWorkedWith_Lisp',\n",
    "        'LanguageHaveWorkedWith_Lua', 'LanguageHaveWorkedWith_MATLAB','LanguageHaveWorkedWith_Nim', \n",
    "        'LanguageHaveWorkedWith_OCaml','LanguageHaveWorkedWith_Objective-C','LanguageHaveWorkedWith_PHP', \n",
    "        'LanguageHaveWorkedWith_Perl', 'LanguageHaveWorkedWith_PowerShell', 'LanguageHaveWorkedWith_Prolog',\n",
    "        'LanguageHaveWorkedWith_Python', 'LanguageHaveWorkedWith_R', 'LanguageHaveWorkedWith_Ruby', \n",
    "        'LanguageHaveWorkedWith_Rust', 'LanguageHaveWorkedWith_SQL', 'LanguageHaveWorkedWith_Scala', \n",
    "        'LanguageHaveWorkedWith_Solidity', 'LanguageHaveWorkedWith_Swift', 'LanguageHaveWorkedWith_TypeScript', \n",
    "        'LanguageHaveWorkedWith_VBA', 'LanguageHaveWorkedWith_Visual Basic (.Net)', \n",
    "        'LanguageHaveWorkedWith_Zig', 'LanguageWantToWorkWith_', 'LanguageWantToWorkWith_Ada', \n",
    "        'LanguageWantToWorkWith_Apex', 'LanguageWantToWorkWith_Assembly', 'LanguageWantToWorkWith_Bash/Shell (all shells)',\n",
    "        'LanguageWantToWorkWith_C', 'LanguageWantToWorkWith_C#', 'LanguageWantToWorkWith_C++', \n",
    "        'LanguageWantToWorkWith_Clojure', 'LanguageWantToWorkWith_Crystal', 'LanguageWantToWorkWith_Dart', \n",
    "        'LanguageWantToWorkWith_Delphi', 'LanguageWantToWorkWith_Elixir', 'LanguageWantToWorkWith_Erlang', \n",
    "        'LanguageWantToWorkWith_F#', 'LanguageWantToWorkWith_Fortran', 'LanguageWantToWorkWith_GDScript', \n",
    "        'LanguageWantToWorkWith_Go', 'LanguageWantToWorkWith_Groovy', 'LanguageWantToWorkWith_HTML/CSS',\n",
    "        'LanguageWantToWorkWith_Haskell', 'LanguageWantToWorkWith_Java', 'LanguageWantToWorkWith_JavaScript', \n",
    "        'LanguageWantToWorkWith_Julia', 'LanguageWantToWorkWith_Kotlin', 'LanguageWantToWorkWith_Lisp', \n",
    "        'LanguageWantToWorkWith_Lua', 'LanguageWantToWorkWith_MATLAB', 'LanguageWantToWorkWith_Nim', \n",
    "        'LanguageWantToWorkWith_OCaml', 'LanguageWantToWorkWith_Objective-C', 'LanguageWantToWorkWith_PHP', \n",
    "        'LanguageWantToWorkWith_Perl', 'LanguageWantToWorkWith_PowerShell', \n",
    "        'LanguageWantToWorkWith_Prolog', 'LanguageWantToWorkWith_Python','LanguageWantToWorkWith_R',\n",
    "        'LanguageWantToWorkWith_Ruby', 'LanguageWantToWorkWith_Rust', 'LanguageWantToWorkWith_SQL', \n",
    "        'LanguageWantToWorkWith_Scala','LanguageWantToWorkWith_Solidity', 'LanguageWantToWorkWith_Swift', \n",
    "        'LanguageWantToWorkWith_TypeScript', 'LanguageWantToWorkWith_VBA', \n",
    "        'LanguageWantToWorkWith_Visual Basic (.Net)','LanguageWantToWorkWith_Zig', \n",
    "        'DatabaseHaveWorkedWith_', 'DatabaseHaveWorkedWith_BigQuery', 'DatabaseHaveWorkedWith_Cassandra', \n",
    "        'DatabaseHaveWorkedWith_Clickhouse', 'DatabaseHaveWorkedWith_Cloud Firestore', \n",
    "        'DatabaseHaveWorkedWith_Cockroachdb', 'DatabaseHaveWorkedWith_Cosmos DB', \n",
    "        'DatabaseHaveWorkedWith_Couch DB', 'DatabaseHaveWorkedWith_Couchbase', \n",
    "        'DatabaseHaveWorkedWith_Datomic', 'DatabaseHaveWorkedWith_DuckDB', 'DatabaseHaveWorkedWith_Dynamodb', \n",
    "        'DatabaseHaveWorkedWith_Elasticsearch', 'DatabaseHaveWorkedWith_Firebase Realtime Database', \n",
    "        'DatabaseHaveWorkedWith_Firebird', 'DatabaseHaveWorkedWith_H2', 'DatabaseHaveWorkedWith_IBM DB2', \n",
    "        'DatabaseHaveWorkedWith_InfluxDB', 'DatabaseHaveWorkedWith_MariaDB' , 'DatabaseHaveWorkedWith_Microsoft Access', \n",
    "        'DatabaseHaveWorkedWith_Microsoft SQL Server', 'DatabaseHaveWorkedWith_MongoDB', 'DatabaseHaveWorkedWith_MySQL', \n",
    "        'DatabaseHaveWorkedWith_Neo4J', 'DatabaseHaveWorkedWith_Oracle', 'DatabaseHaveWorkedWith_PostgreSQL', \n",
    "        'DatabaseHaveWorkedWith_RavenDB', 'DatabaseHaveWorkedWith_Redis', 'DatabaseHaveWorkedWith_SQLite', \n",
    "        'DatabaseHaveWorkedWith_Snowflake', 'DatabaseHaveWorkedWith_Solr', 'DatabaseHaveWorkedWith_Supabase',\n",
    "        'ToolsTechHaveWorkedWith_', 'ToolsTechHaveWorkedWith_APT', 'ToolsTechHaveWorkedWith_Ansible', \n",
    "        'ToolsTechHaveWorkedWith_Ant', 'ToolsTechHaveWorkedWith_Bun', 'ToolsTechHaveWorkedWith_Chef', \n",
    "        'ToolsTechHaveWorkedWith_Chocolatey', 'ToolsTechHaveWorkedWith_Composer', 'ToolsTechHaveWorkedWith_Dagger', \n",
    "        'ToolsTechHaveWorkedWith_Docker', 'ToolsTechHaveWorkedWith_Godot','ToolsTechHaveWorkedWith_Google Test', \n",
    "        'ToolsTechHaveWorkedWith_Gradle', 'ToolsTechHaveWorkedWith_Homebrew', 'ToolsTechHaveWorkedWith_Kubernetes', \n",
    "        'ToolsTechHaveWorkedWith_MSBuild', 'ToolsTechHaveWorkedWith_Make', 'ToolsTechHaveWorkedWith_Maven (build tool)', \n",
    "        'ToolsTechHaveWorkedWith_Ninja', 'ToolsTechHaveWorkedWith_Nix', 'ToolsTechHaveWorkedWith_NuGet', 'ToolsTechHaveWorkedWith_Pacman', \n",
    "        'ToolsTechHaveWorkedWith_Pip', 'ToolsTechHaveWorkedWith_Podman', 'ToolsTechHaveWorkedWith_Pulumi', 'ToolsTechHaveWorkedWith_Puppet', \n",
    "        'ToolsTechHaveWorkedWith_Terraform', 'ToolsTechHaveWorkedWith_Unity 3D', 'ToolsTechHaveWorkedWith_Unreal Engine', \n",
    "        'ToolsTechHaveWorkedWith_Visual Studio Solution', 'ToolsTechHaveWorkedWith_Vite', 'ToolsTechHaveWorkedWith_Webpack',\n",
    "          'ToolsTechHaveWorkedWith_Yarn', 'ToolsTechHaveWorkedWith_npm', 'ToolsTechHaveWorkedWith_pnpm']]\n",
    "# Dividir los datos en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primer modelo: baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Métricas de Cross-Validation ===\n",
      "R² (promedio CV): 0.4857\n",
      "MSE (promedio CV): 69922151.1770\n",
      "RMSE (promedio CV): 8361.9466\n",
      "MAE (promedio CV): 6358.2760\n",
      "MAPE (promedio CV): 17.0885%\n"
     ]
    }
   ],
   "source": [
    "# Asegúrate de convertir los arrays de numpy a pandas DataFrame y Series si es necesario\n",
    "X_train = pd.DataFrame(X_train)  # Si X_train es un ndarray, conviértelo en DataFrame\n",
    "y_train = pd.Series(y_train)  # Si y_train es un ndarray, conviértelo en Series\n",
    "\n",
    "# Crear un modelo de Random Forest\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "cv = 5  # Número de folds\n",
    "\n",
    "# Inicializamos listas para guardar las métricas de cada fold\n",
    "r2_scores = []\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "mape_scores = []\n",
    "\n",
    "# Realizar la validación cruzada\n",
    "kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]  # Usar .iloc con DataFrame\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]  # Usar .iloc con Series\n",
    "    \n",
    "    # Entrenar el modelo en el fold\n",
    "    rf_model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predecir en el conjunto de validación\n",
    "    y_val_pred_log = rf_model.predict(X_val_fold)\n",
    "    \n",
    "    # Revertir la transformación logarítmica\n",
    "    y_val_pred = np.expm1(y_val_pred_log)\n",
    "    y_val_actual = np.expm1(y_val_fold)\n",
    "    \n",
    "    # Calcular las métricas para este fold\n",
    "    r2 = r2_score(y_val_actual, y_val_pred)\n",
    "    mse = mean_squared_error(y_val_actual, y_val_pred)\n",
    "    mae = mean_absolute_error(y_val_actual, y_val_pred)\n",
    "    mape = np.mean(np.abs((y_val_actual - y_val_pred) / y_val_actual)) * 100  \n",
    "    \n",
    "    # Guardar las métricas en las listas\n",
    "    r2_scores.append(r2)\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    mape_scores.append(mape)  # Guardar MAPE\n",
    "\n",
    "# Promedios de métricas en CV\n",
    "mean_r2_cv = np.mean(r2_scores)\n",
    "mean_mse_cv = np.mean(mse_scores)\n",
    "mean_rmse_cv = np.sqrt(mean_mse_cv)\n",
    "mean_mae_cv = np.mean(mae_scores)\n",
    "mean_mape_cv = np.mean(mape_scores)  # Promedio de MAPE\n",
    "\n",
    "# Métricas de Cross-Validation\n",
    "print(\"=== Métricas de Cross-Validation ===\")\n",
    "print(f\"R² (promedio CV): {mean_r2_cv:.4f}\")\n",
    "print(f\"MSE (promedio CV): {mean_mse_cv:.4f}\")\n",
    "print(f\"RMSE (promedio CV): {mean_rmse_cv:.4f}\")\n",
    "print(f\"MAE (promedio CV): {mean_mae_cv:.4f}\")\n",
    "print(f\"MAPE (promedio CV): {mean_mape_cv:.4f}%\")  # Imprimir MAPE promedio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== Métricas de Cross-Validation ===\n",
    "R² (promedio CV): 0.4857\n",
    "MSE (promedio CV): 69922151.1770\n",
    "RMSE (promedio CV): 8361.9466\n",
    "MAE (promedio CV): 6358.2760\n",
    "MAPE (promedio CV): 17.0885%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Linear Regressor da bastante bien, pruebo regularizandolo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor valor de alpha: 10.0\n",
      "=== Métricas de Cross-Validation (Ridge) ===\n",
      "R² (promedio CV): 0.2241\n",
      "MSE (promedio CV): 105467197.6167\n",
      "RMSE (promedio CV): 10257.7060\n",
      "MAE (promedio CV): 8078.5939\n",
      "MAPE (promedio CV): 22.0213%\n"
     ]
    }
   ],
   "source": [
    "# Asegúrate de que X_train y y_train sean DataFrame y Series de pandas\n",
    "X_train = pd.DataFrame(X_train)\n",
    "y_train = pd.Series(y_train)\n",
    "\n",
    "# Definir el rango de valores de alpha para la búsqueda en cuadrícula\n",
    "param_grid = {'alpha': np.logspace(-6, 6, 13)}  # Valores de alpha entre 0.0001 y 10000\n",
    "\n",
    "# Inicialización del modelo Ridge\n",
    "ridge_model = Ridge(random_state=42)\n",
    "\n",
    "# Configuración de GridSearchCV para encontrar el mejor alpha\n",
    "grid_search = GridSearchCV(estimator=ridge_model, param_grid=param_grid, \n",
    "                           scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
    "\n",
    "# Ajustar el modelo a los datos de entrenamiento\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Imprimir el mejor valor de alpha\n",
    "print(f\"Mejor valor de alpha: {grid_search.best_params_['alpha']}\")\n",
    "\n",
    "# Mejor modelo después de GridSearchCV\n",
    "best_ridge_model = grid_search.best_estimator_\n",
    "\n",
    "# Realizar la validación cruzada\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Listas para almacenar las métricas de cada fold\n",
    "r2_scores = []\n",
    "mse_scores = []\n",
    "rmse_scores = []\n",
    "mae_scores = []\n",
    "mape_scores = []\n",
    "\n",
    "for train_index, val_index in cv.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Entrenar el mejor modelo Ridge en el fold actual\n",
    "    best_ridge_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Hacer predicciones en el fold de validación (en la escala logarítmica)\n",
    "    y_val_pred_log = best_ridge_model.predict(X_val_fold)\n",
    "\n",
    "    # Aplicar la transformación inversa a las predicciones y a los valores reales\n",
    "    y_val_pred = np.expm1(y_val_pred_log)\n",
    "    y_val_original = np.expm1(y_val_fold)\n",
    "\n",
    "    # Calcular métricas en la escala original\n",
    "    r2_scores.append(r2_score(y_val_original, y_val_pred))\n",
    "    mse_scores.append(mean_squared_error(y_val_original, y_val_pred))\n",
    "    rmse_scores.append(np.sqrt(mse_scores[-1]))\n",
    "    mae_scores.append(mean_absolute_error(y_val_original, y_val_pred))\n",
    "    mape_scores.append(np.mean(np.abs((y_val_original - y_val_pred) / y_val_original)) * 100)\n",
    "\n",
    "# Calcular los promedios de las métricas\n",
    "mean_r2_cv = np.mean(r2_scores)\n",
    "mean_mse_cv = np.mean(mse_scores)\n",
    "mean_rmse_cv = np.mean(rmse_scores)\n",
    "mean_mae_cv = np.mean(mae_scores)\n",
    "mean_mape_cv = np.mean(mape_scores)\n",
    "\n",
    "# Imprimir las métricas\n",
    "print(\"=== Métricas de Cross-Validation (Ridge) ===\")\n",
    "print(f\"R² (promedio CV): {mean_r2_cv:.4f}\")\n",
    "print(f\"MSE (promedio CV): {mean_mse_cv:.4f}\")\n",
    "print(f\"RMSE (promedio CV): {mean_rmse_cv:.4f}\")\n",
    "print(f\"MAE (promedio CV): {mean_mae_cv:.4f}\")\n",
    "print(f\"MAPE (promedio CV): {mean_mape_cv:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No funciona el linear regressor, las relaciones entre los datos son mas complejas. Probamos con Modelos complejos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebo Gridsearch de Random Forest para mejorar los hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Mejores parámetros: {'max_depth': 9, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "=== Métricas de Cross-Validation ===\n",
      "R² (promedio CV): 0.4668\n",
      "MSE (promedio CV): 72587400.2487\n",
      "RMSE (promedio CV): 8519.8240\n",
      "MAE (promedio CV): 6466.5447\n",
      "MAPE (promedio CV): 17.3979%\n"
     ]
    }
   ],
   "source": [
    "#Asegúrate de convertir los arrays a DataFrame y Series si es necesario\n",
    "X_train = pd.DataFrame(X_train)  # Si X_train es un ndarray, conviértelo en DataFrame\n",
    "y_train = pd.Series(y_train)  # Si y_train es un ndarray, conviértelo en Series\n",
    "\n",
    "# Definir MAPE como función\n",
    "def mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Configurar el grid de parámetros\n",
    "param_grid = {\n",
    "    'n_estimators': [250, 300],\n",
    "    'max_depth': [5, 7, 9],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [1]\n",
    "}\n",
    "\n",
    "# Crear un modelo de Random Forest\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Crear el objeto GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring={\n",
    "        'r2': 'r2',\n",
    "        'neg_mse': 'neg_mean_squared_error',\n",
    "        'neg_mae': 'neg_mean_absolute_error',\n",
    "        'neg_mape': make_scorer(mape, greater_is_better=False)\n",
    "    },\n",
    "    refit='neg_mape',  # Ajustar el mejor modelo basado en MAPE\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Número de folds para validación cruzada\n",
    "cv = 3\n",
    "\n",
    "# Inicializamos listas para guardar las métricas de cada fold\n",
    "r2_scores = []\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "mape_scores = []\n",
    "\n",
    "# Crear un objeto KFold para dividir los datos\n",
    "kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "# Realizar validación cruzada manualmente\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    # Dividir los datos en folds de entrenamiento y validación\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "    \n",
    "    # Ajustar el modelo en el fold actual\n",
    "    grid_search.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predecir en el conjunto de validación\n",
    "    y_val_pred_log = grid_search.predict(X_val_fold)\n",
    "    \n",
    "    # Revertir la transformación logarítmica (si los datos se transformaron previamente)\n",
    "    y_val_pred = np.expm1(y_val_pred_log)\n",
    "    y_val_actual = np.expm1(y_val_fold)\n",
    "    \n",
    "    # Calcular las métricas para este fold\n",
    "    r2 = r2_score(y_val_actual, y_val_pred)\n",
    "    mse = mean_squared_error(y_val_actual, y_val_pred)\n",
    "    mae = mean_absolute_error(y_val_actual, y_val_pred)\n",
    "    mape_value = mape(y_val_actual, y_val_pred)\n",
    "\n",
    "    # Guardar las métricas en las listas\n",
    "    r2_scores.append(r2)\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    mape_scores.append(mape_value)\n",
    "\n",
    "# Obtener el mejor modelo\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtener los mejores parámetros\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Mejores parámetros:\", best_params)\n",
    "\n",
    "# Calcular las métricas promedio de la validación cruzada\n",
    "mean_r2_cv = np.mean(r2_scores)\n",
    "mean_mse_cv = np.mean(mse_scores)\n",
    "mean_rmse_cv = np.sqrt(mean_mse_cv)\n",
    "mean_mae_cv = np.mean(mae_scores)\n",
    "mean_mape_cv = np.mean(mape_scores)\n",
    "\n",
    "# Imprimir métricas promedio\n",
    "print(\"=== Métricas de Cross-Validation ===\")\n",
    "print(f\"R² (promedio CV): {mean_r2_cv:.4f}\")\n",
    "print(f\"MSE (promedio CV): {mean_mse_cv:.4f}\")\n",
    "print(f\"RMSE (promedio CV): {mean_rmse_cv:.4f}\")\n",
    "print(f\"MAE (promedio CV): {mean_mae_cv:.4f}\")\n",
    "print(f\"MAPE (promedio CV): {mean_mape_cv:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebo con XGBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Mejores parámetros: {'colsample_bytree': 0.9, 'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.7}\n",
      "=== Métricas de Cross-Validation ===\n",
      "R² (promedio CV): 0.4668\n",
      "MSE (promedio CV): 72584789.3381\n",
      "RMSE (promedio CV): 8519.6707\n",
      "MAE (promedio CV): 6483.5560\n",
      "MAPE (promedio CV): 17.4779%\n"
     ]
    }
   ],
   "source": [
    "# Asegúrate de que X_train y y_train sean DataFrame y Series de pandas\n",
    "X_train = pd.DataFrame(X_train)  # Si X_train es un ndarray, conviértelo en DataFrame\n",
    "y_train = pd.Series(y_train)  # Si y_train es un ndarray, conviértelo en Series\n",
    "\n",
    "# Definir el grid de hiperparámetros para GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [200],\n",
    "    'learning_rate': [0.1],\n",
    "    'max_depth': [5],\n",
    "    'min_child_weight': [3],\n",
    "    'subsample': [0.7],\n",
    "    'colsample_bytree': [0.9],\n",
    "    'gamma': [0.1],\n",
    "}\n",
    "\n",
    "# Crear un modelo de Random Forest\n",
    "xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Configuración de GridSearchCV con validación cruzada\n",
    "grid_search = GridSearchCV(estimator=xgb_model, \n",
    "                           param_grid=param_grid, \n",
    "                           scoring='neg_mean_absolute_error', \n",
    "                           cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "cv = 3  # Número de folds\n",
    "\n",
    "# Inicializamos listas para guardar las métricas de cada fold\n",
    "r2_scores = []\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "mape_scores = []\n",
    "\n",
    "# Realizar la validación cruzada\n",
    "kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]  # Usar .iloc con DataFrame\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]  # Usar .iloc con Series\n",
    "    \n",
    "    # Entrenar el modelo en el fold\n",
    "    grid_search.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predecir en el conjunto de validación\n",
    "    y_val_pred_log = grid_search.predict(X_val_fold)\n",
    "    \n",
    "    # Revertir la transformación logarítmica\n",
    "    y_val_pred = np.expm1(y_val_pred_log)\n",
    "    y_val_actual = np.expm1(y_val_fold)\n",
    "    \n",
    "    # Calcular las métricas para este fold\n",
    "    r2 = r2_score(y_val_actual, y_val_pred)\n",
    "    mse = mean_squared_error(y_val_actual, y_val_pred)\n",
    "    mae = mean_absolute_error(y_val_actual, y_val_pred)\n",
    "    mape = np.mean(np.abs((y_val_actual - y_val_pred) / y_val_actual)) * 100\n",
    "\n",
    "    # Guardar las métricas en las listas\n",
    "    r2_scores.append(r2)\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    mape_scores.append(mape) \n",
    "# Obtener el mejor modelo\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtener los mejores parámetros\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Mejores parámetros:\", best_params)\n",
    "\n",
    "# Promedios de métricas en CV\n",
    "mean_r2_cv = np.mean(r2_scores)\n",
    "mean_mse_cv = np.mean(mse_scores)\n",
    "mean_rmse_cv = np.sqrt(mean_mse_cv)\n",
    "mean_mae_cv = np.mean(mae_scores)\n",
    "mean_mape_cv = np.mean(mape_scores) \n",
    "\n",
    "# Métricas de Cross-Validation\n",
    "print(\"=== Métricas de Cross-Validation ===\")\n",
    "print(f\"R² (promedio CV): {mean_r2_cv:.4f}\")\n",
    "print(f\"MSE (promedio CV): {mean_mse_cv:.4f}\")\n",
    "print(f\"RMSE (promedio CV): {mean_rmse_cv:.4f}\")\n",
    "print(f\"MAE (promedio CV): {mean_mae_cv:.4f}\")\n",
    "print(f\"MAPE (promedio CV): {mean_mape_cv:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebo con Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 1 is smaller than n_iter=100. Running 1 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Mejores parámetros: {'subsample': 0.9, 'n_estimators': 450, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 5, 'learning_rate': 0.1, 'ccp_alpha': 0.0, 'alpha': 0.1}\n",
      "=== Métricas de Cross-Validation ===\n",
      "R² (promedio CV): 0.4178\n",
      "MSE (promedio CV): 79269133.6352\n",
      "RMSE (promedio CV): 8903.3215\n",
      "MAE (promedio CV): 6901.1346\n",
      "MAPE (promedio CV): 18.6091%\n"
     ]
    }
   ],
   "source": [
    "# Asegúrate de convertir los arrays de numpy a pandas DataFrame y Series si es necesario\n",
    "X_train = pd.DataFrame(X_train)  # Si X_train es un ndarray, conviértelo en DataFrame\n",
    "y_train = pd.Series(y_train)  # Si y_train es un ndarray, conviértelo en Series\n",
    "\n",
    "# --- Definir el Espacio de Búsqueda de Hiperparámetros ---\n",
    "param_distributions = {\n",
    "    'n_estimators': [450],\n",
    "    'learning_rate': [0.1],\n",
    "    'max_depth': [5],\n",
    "    'min_samples_split': [10],\n",
    "    'min_samples_leaf': [2],\n",
    "    'subsample': [0.9],\n",
    "    'max_features': ['sqrt'],\n",
    "    'alpha': [0.1], #parametro de regularizacion L1\n",
    "    'ccp_alpha': [0.0] #parametro para la poda\n",
    "}\n",
    "\n",
    "# --- Instanciar el Modelo y RandomizedSearchCV ---\n",
    "gbr = GradientBoostingRegressor(random_state=42, loss='squared_error') \n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=gbr,  # Cambiar a GradientBoostingRegressor\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=100,  # Número de combinaciones a probar\n",
    "    cv=3,  # Número de folds en la validación cruzada\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # Usar todos los procesadores disponibles\n",
    "    scoring='neg_mean_squared_error'  # Puedes usar otra métrica si lo prefieres\n",
    ")\n",
    "cv = 3  # Número de folds\n",
    "\n",
    "# Inicializamos listas para guardar las métricas de cada fold\n",
    "r2_scores = []\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "mape_scores = []\n",
    "\n",
    "# Realizar la validación cruzada\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]  # Usar .iloc con DataFrame\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]  # Usar .iloc con Series\n",
    "    \n",
    "    # Entrenar el modelo en el fold\n",
    "    random_search.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predecir en el conjunto de validación\n",
    "    y_val_pred_log = random_search.predict(X_val_fold)\n",
    "    \n",
    "    # Revertir la transformación logarítmica\n",
    "    y_val_pred = np.expm1(y_val_pred_log)\n",
    "    y_val_actual = np.expm1(y_val_fold)\n",
    "    \n",
    "    # Calcular las métricas para este fold\n",
    "    r2 = r2_score(y_val_actual, y_val_pred)\n",
    "    mse = mean_squared_error(y_val_actual, y_val_pred)\n",
    "    mae = mean_absolute_error(y_val_actual, y_val_pred)\n",
    "    mape = np.mean(np.abs((y_val_actual - y_val_pred) / y_val_actual)) * 100\n",
    "\n",
    "    # Guardar las métricas en las listas\n",
    "    r2_scores.append(r2)\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    mape_scores.append(mape) \n",
    "# Obtener el mejor modelo\n",
    "best_rf_model = random_search.best_estimator_\n",
    "\n",
    "# Obtener los mejores parámetros\n",
    "best_params = random_search.best_params_\n",
    "print(\"Mejores parámetros:\", best_params)\n",
    "\n",
    "# Promedios de métricas en CV\n",
    "mean_r2_cv = np.mean(r2_scores)\n",
    "mean_mse_cv = np.mean(mse_scores)\n",
    "mean_rmse_cv = np.sqrt(mean_mse_cv)\n",
    "mean_mae_cv = np.mean(mae_scores)\n",
    "mean_mape_cv = np.mean(mape_scores) \n",
    "\n",
    "# Métricas de Cross-Validation\n",
    "print(\"=== Métricas de Cross-Validation ===\")\n",
    "print(f\"R² (promedio CV): {mean_r2_cv:.4f}\")\n",
    "print(f\"MSE (promedio CV): {mean_mse_cv:.4f}\")\n",
    "print(f\"RMSE (promedio CV): {mean_rmse_cv:.4f}\")\n",
    "print(f\"MAE (promedio CV): {mean_mae_cv:.4f}\")\n",
    "print(f\"MAPE (promedio CV): {mean_mape_cv:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "Mejores parámetros: {'colsample_bytree': 0.9, 'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.7}\n",
      "=== Métricas de Cross-Validation ===\n",
      "R² (promedio CV): 0.4668\n",
      "MSE (promedio CV): 72584789.3381\n",
      "RMSE (promedio CV): 8519.6707\n",
      "MAE (promedio CV): 6483.5560\n",
      "MAPE (promedio CV): 17.4779%\n"
     ]
    }
   ],
   "source": [
    "# Asegúrate de que X_train y y_train sean DataFrame y Series de pandas\n",
    "X_train = pd.DataFrame(X_train)  # Si X_train es un ndarray, conviértelo en DataFrame\n",
    "y_train = pd.Series(y_train)  # Si y_train es un ndarray, conviértelo en Series\n",
    "# Definir el grid de hiperparámetros para GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [200],\n",
    "    'learning_rate': [0.1],\n",
    "    'max_depth': [5],\n",
    "    'min_child_weight': [3],\n",
    "    'subsample': [0.7],\n",
    "    'colsample_bytree': [0.9],\n",
    "    'gamma': [0.1],\n",
    "}\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('polynomialfeatures', PolynomialFeatures(include_bias=False)),  # Excluir bias para evitar columnas redundantes\n",
    "    ('xgb_model', XGBRegressor(random_state=42, objective='reg:squarederror'))\n",
    "])\n",
    "\n",
    "\n",
    "# Configuración de GridSearchCV con validación cruzada\n",
    "grid_search = GridSearchCV(estimator=xgb_model, \n",
    "                           param_grid=param_grid, \n",
    "                           scoring='neg_mean_absolute_error', \n",
    "                           cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "cv = 3  # Número de folds\n",
    "\n",
    "# Inicializamos listas para guardar las métricas de cada fold\n",
    "r2_scores = []\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "mape_scores = []\n",
    "\n",
    "# Realizar la validación cruzada\n",
    "kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]  # Usar .iloc con DataFrame\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]  # Usar .iloc con Series\n",
    "    \n",
    "    # Entrenar el modelo en el fold\n",
    "    grid_search.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predecir en el conjunto de validación\n",
    "    y_val_pred_log = grid_search.predict(X_val_fold)\n",
    "    \n",
    "    # Revertir la transformación logarítmica\n",
    "    y_val_pred = np.expm1(y_val_pred_log)\n",
    "    y_val_actual = np.expm1(y_val_fold)\n",
    "    \n",
    "    # Calcular las métricas para este fold\n",
    "    r2 = r2_score(y_val_actual, y_val_pred)\n",
    "    mse = mean_squared_error(y_val_actual, y_val_pred)\n",
    "    mae = mean_absolute_error(y_val_actual, y_val_pred)\n",
    "    mape = np.mean(np.abs((y_val_actual - y_val_pred) / y_val_actual)) * 100\n",
    "\n",
    "    # Guardar las métricas en las listas\n",
    "    r2_scores.append(r2)\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    mape_scores.append(mape) \n",
    "# Obtener el mejor modelo\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtener los mejores parámetros\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Mejores parámetros:\", best_params)\n",
    "\n",
    "# Promedios de métricas en CV\n",
    "mean_r2_cv = np.mean(r2_scores)\n",
    "mean_mse_cv = np.mean(mse_scores)\n",
    "mean_rmse_cv = np.sqrt(mean_mse_cv)\n",
    "mean_mae_cv = np.mean(mae_scores)\n",
    "mean_mape_cv = np.mean(mape_scores) \n",
    "\n",
    "# Métricas de Cross-Validation\n",
    "print(\"=== Métricas de Cross-Validation ===\")\n",
    "print(f\"R² (promedio CV): {mean_r2_cv:.4f}\")\n",
    "print(f\"MSE (promedio CV): {mean_mse_cv:.4f}\")\n",
    "print(f\"RMSE (promedio CV): {mean_rmse_cv:.4f}\")\n",
    "print(f\"MAE (promedio CV): {mean_mae_cv:.4f}\")\n",
    "print(f\"MAPE (promedio CV): {mean_mape_cv:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Métricas de Cross-Validation ===\n",
      "R² (promedio CV): 0.4849\n",
      "MSE (promedio CV): 70141544.4914\n",
      "RMSE (promedio CV): 8375.0549\n",
      "MAE (promedio CV): 6416.7508\n",
      "MAPE (promedio CV): 17.2768%\n",
      "=== Métricas de Test ===\n",
      "R² (Test): 0.5692\n",
      "MSE (Test): 63936946.7426\n",
      "RMSE (Test): 7996.0582\n",
      "MAE (Test): 6172.9491\n",
      "MAPE (Test): 16.6868%\n"
     ]
    }
   ],
   "source": [
    "# Asegúrate de convertir los arrays de numpy a pandas DataFrame y Series si es necesario\n",
    "X_train = pd.DataFrame(X_train)  # Si X_train es un ndarray, conviértelo en DataFrame\n",
    "y_train = pd.Series(y_train)  # Si y_train es un ndarray, conviértelo en Series\n",
    "# Definir tus mejores modelos\n",
    "random_forest = RandomForestRegressor(max_depth=9, min_samples_leaf = 1, min_samples_split= 2, n_estimators=300)\n",
    "xgb_reg = XGBRegressor(objective='reg:squarederror', random_state=42, colsample_bytree= 0.9, learning_rate= 0.1, max_depth= 5, min_child_weight= 5, n_estimators= 200, reg_alpha= 0.1, reg_lambda= 0.1, subsample= 0.7, gamma=0.1)\n",
    "gb_model = GradientBoostingRegressor(subsample =0.9, min_samples_split = 10, n_estimators=450, learning_rate=0.1, min_samples_leaf = 2, max_depth=5, random_state=42, loss='squared_error', ccp_alpha = 0.0, alpha= 0.1, max_features ='sqrt')\n",
    "# Crear el ensemble\n",
    "voting_regressor = VotingRegressor(estimators=[('rf', random_forest), ('xgb', xgb_reg), ('gb', gb_model)])\n",
    "\n",
    "# Ajustar el ensemble al conjunto de entrenamiento\n",
    "voting_regressor.fit(X_train, y_train)\n",
    "\n",
    "cv = 3  # Número de folds\n",
    "\n",
    "# Inicializamos listas para guardar las métricas de cada fold\n",
    "r2_scores = []\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "mape_scores = []\n",
    "\n",
    "# Realizar la validación cruzada\n",
    "kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]  # Usar .iloc con DataFrame\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]  # Usar .iloc con Series\n",
    "    \n",
    "    # Entrenar el modelo en el fold\n",
    "    voting_regressor.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predecir en el conjunto de validación\n",
    "    y_val_pred_log = voting_regressor.predict(X_val_fold)\n",
    "    \n",
    "    # Revertir la transformación logarítmica\n",
    "    y_val_pred = np.expm1(y_val_pred_log)\n",
    "    y_val_actual = np.expm1(y_val_fold)\n",
    "    \n",
    "    # Calcular las métricas para este fold\n",
    "    r2 = r2_score(y_val_actual, y_val_pred)\n",
    "    mse = mean_squared_error(y_val_actual, y_val_pred)\n",
    "    mae = mean_absolute_error(y_val_actual, y_val_pred)\n",
    "    mape = np.mean(np.abs((y_val_actual - y_val_pred) / y_val_actual)) * 100\n",
    "\n",
    "    # Guardar las métricas en las listas\n",
    "    r2_scores.append(r2)\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    mape_scores.append(mape) \n",
    "\n",
    "# Promedios de métricas en CV\n",
    "mean_r2_cv = np.mean(r2_scores)\n",
    "mean_mse_cv = np.mean(mse_scores)\n",
    "mean_rmse_cv = np.sqrt(mean_mse_cv)\n",
    "mean_mae_cv = np.mean(mae_scores)\n",
    "mean_mape_cv = np.mean(mape_scores) \n",
    "\n",
    "# Métricas de Cross-Validation\n",
    "print(\"=== Métricas de Cross-Validation ===\")\n",
    "print(f\"R² (promedio CV): {mean_r2_cv:.4f}\")\n",
    "print(f\"MSE (promedio CV): {mean_mse_cv:.4f}\")\n",
    "print(f\"RMSE (promedio CV): {mean_rmse_cv:.4f}\")\n",
    "print(f\"MAE (promedio CV): {mean_mae_cv:.4f}\")\n",
    "print(f\"MAPE (promedio CV): {mean_mape_cv:.4f}%\")\n",
    "\n",
    "# Predicciones con el conjunto de test\n",
    "y_test_pred_log = voting_regressor.predict(X_test)\n",
    "\n",
    "# Revertir la transformación logarítmica para las predicciones del test\n",
    "y_test_pred = np.expm1(y_test_pred_log)\n",
    "y_test_actual = np.expm1(y_test)\n",
    "\n",
    "# Calcular las métricas para el conjunto de test\n",
    "r2_test = r2_score(y_test_actual, y_test_pred)\n",
    "mse_test = mean_squared_error(y_test_actual, y_test_pred)\n",
    "mae_test = mean_absolute_error(y_test_actual, y_test_pred)\n",
    "mape_test = np.mean(np.abs((y_test_actual - y_test_pred) / y_test_actual)) * 100\n",
    "\n",
    "# Imprimir métricas de test\n",
    "print(\"=== Métricas de Test ===\")\n",
    "print(f\"R² (Test): {r2_test:.4f}\")\n",
    "print(f\"MSE (Test): {mse_test:.4f}\")\n",
    "print(f\"RMSE (Test): {np.sqrt(mse_test):.4f}\")\n",
    "print(f\"MAE (Test): {mae_test:.4f}\")\n",
    "print(f\"MAPE (Test): {mape_test:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metricas con techo de 55k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Métricas de Cross-Validation ===\n",
      "R² (promedio CV): 0.4787\n",
      "MSE (promedio CV): 54552814.0755\n",
      "RMSE (promedio CV): 7385.9877\n",
      "MAE (promedio CV): 5693.6104\n",
      "MAPE (promedio CV): 16.2198%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Asegúrate de convertir los arrays de numpy a pandas DataFrame y Series si es necesario\n",
    "X_train = pd.DataFrame(X_train)  # Si X_train es un ndarray, conviértelo en DataFrame\n",
    "y_train = pd.Series(y_train)  # Si y_train es un ndarray, conviértelo en Series\n",
    "X_test = pd.DataFrame(X_test)  # Lo mismo para el conjunto de test\n",
    "y_test = pd.Series(y_test)\n",
    "\n",
    "# Definir tus mejores modelos\n",
    "random_forest = RandomForestRegressor(max_depth=9, min_samples_leaf=1, min_samples_split=2, n_estimators=300)\n",
    "xgb_reg = XGBRegressor(objective='reg:squarederror', random_state=42, colsample_bytree=0.9, learning_rate=0.1,\n",
    "                       max_depth=5, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.1,\n",
    "                       subsample=0.7, gamma=0.1)\n",
    "gb_model = GradientBoostingRegressor(subsample=0.9, min_samples_split=10, n_estimators=450, learning_rate=0.1,\n",
    "                                     min_samples_leaf=2, max_depth=5, random_state=42, loss='squared_error',\n",
    "                                     ccp_alpha=0.0, alpha=0.1, max_features='sqrt')\n",
    "\n",
    "# Crear el ensemble\n",
    "voting_regressor = VotingRegressor(estimators=[('rf', random_forest), ('xgb', xgb_reg), ('gb', gb_model)])\n",
    "\n",
    "# Ajustar el ensemble al conjunto de entrenamiento\n",
    "voting_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Entrenar los modelos base por separado\n",
    "random_forest.fit(X_train, y_train)\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Obtener la importancia de las características de los modelos base\n",
    "rf_importances = random_forest.feature_importances_\n",
    "xgb_importances = xgb_reg.feature_importances_\n",
    "gb_importances = gb_model.feature_importances_\n",
    "\n",
    "# Promediar las importancias\n",
    "average_importances = (rf_importances + xgb_importances + gb_importances) / 3\n",
    "\n",
    "# Crear un DataFrame con las características y sus importancias\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': average_importances\n",
    "})\n",
    "\n",
    "# Ordenar las características por importancia\n",
    "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Seleccionar las 50 características más importantes\n",
    "top_50_features = feature_importance_df['feature'][:50]\n",
    "\n",
    "# Filtrar X_train y X_test para quedarnos solo con las 50 características más importantes\n",
    "X_train_top50 = X_train[top_50_features]\n",
    "X_test_top50 = X_test[top_50_features]\n",
    "\n",
    "# Re-entrenar el modelo solo con las 50 características más importantes\n",
    "voting_regressor.fit(X_train_top50, y_train)\n",
    "\n",
    "# Realizar la validación cruzada\n",
    "cv = 3  # Número de folds\n",
    "kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "# Inicializamos listas para guardar las métricas de cada fold\n",
    "r2_scores = []\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "mape_scores = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_top50):\n",
    "    X_train_fold, X_val_fold = X_train_top50.iloc[train_index], X_train_top50.iloc[val_index]  # Usar .iloc con DataFrame\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]  # Usar .iloc con Series\n",
    "    \n",
    "    # Entrenar el modelo en el fold\n",
    "    voting_regressor.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predecir en el conjunto de validación\n",
    "    y_val_pred_log = voting_regressor.predict(X_val_fold)\n",
    "    \n",
    "    # Revertir la transformación logarítmica\n",
    "    y_val_pred = np.expm1(y_val_pred_log)\n",
    "    y_val_actual = np.expm1(y_val_fold)\n",
    "    \n",
    "    # Calcular las métricas para este fold\n",
    "    r2 = r2_score(y_val_actual, y_val_pred)\n",
    "    mse = mean_squared_error(y_val_actual, y_val_pred)\n",
    "    mae = mean_absolute_error(y_val_actual, y_val_pred)\n",
    "    mape = np.mean(np.abs((y_val_actual - y_val_pred) / y_val_actual)) * 100\n",
    "\n",
    "    # Guardar las métricas en las listas\n",
    "    r2_scores.append(r2)\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    mape_scores.append(mape)\n",
    "\n",
    "# Promedios de métricas en CV\n",
    "mean_r2_cv = np.mean(r2_scores)\n",
    "mean_mse_cv = np.mean(mse_scores)\n",
    "mean_rmse_cv = np.sqrt(mean_mse_cv)\n",
    "mean_mae_cv = np.mean(mae_scores)\n",
    "mean_mape_cv = np.mean(mape_scores)\n",
    "\n",
    "# Métricas de Cross-Validation\n",
    "print(\"=== Métricas de Cross-Validation ===\")\n",
    "print(f\"R² (promedio CV): {mean_r2_cv:.4f}\")\n",
    "print(f\"MSE (promedio CV): {mean_mse_cv:.4f}\")\n",
    "print(f\"RMSE (promedio CV): {mean_rmse_cv:.4f}\")\n",
    "print(f\"MAE (promedio CV): {mean_mae_cv:.4f}\")\n",
    "print(f\"MAPE (promedio CV): {mean_mape_cv:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=== Métricas de Cross-Validation ===\n",
    "R² (promedio CV): 0.4895\n",
    "MSE (promedio CV): 53445857.2957\n",
    "RMSE (promedio CV): 7310.6674\n",
    "MAE (promedio CV): 5601.1956\n",
    "MAPE (promedio CV): 15.9211%\n",
    "=== Métricas de Test ===\n",
    "R² (Test): 0.5314\n",
    "MSE (Test): 46935010.5253\n",
    "RMSE (Test): 6850.9131\n",
    "MAE (Test): 5366.5108\n",
    "MAPE (Test): 15.7280%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metricas con techo 60k y top 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Métricas de Cross-Validation ===\n",
      "R² (promedio CV): 0.5009\n",
      "MSE (promedio CV): 67969653.5289\n",
      "RMSE (promedio CV): 8244.3710\n",
      "MAE (promedio CV): 6279.4450\n",
      "MAPE (promedio CV): 16.8804%\n",
      "=== Métricas de Test ===\n",
      "R² (Test): 0.5469\n",
      "MSE (Test): 67253644.3578\n",
      "RMSE (Test): 8200.8319\n",
      "MAE (Test): 6314.2274\n",
      "MAPE (Test): 17.0016%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Asegúrate de convertir los arrays de numpy a pandas DataFrame y Series si es necesario\n",
    "X_train = pd.DataFrame(X_train)  # Si X_train es un ndarray, conviértelo en DataFrame\n",
    "y_train = pd.Series(y_train)  # Si y_train es un ndarray, conviértelo en Series\n",
    "X_test = pd.DataFrame(X_test)  # Lo mismo para el conjunto de test\n",
    "y_test = pd.Series(y_test)\n",
    "\n",
    "# Definir tus mejores modelos\n",
    "random_forest = RandomForestRegressor(max_depth=9, min_samples_leaf=1, min_samples_split=2, n_estimators=300)\n",
    "xgb_reg = XGBRegressor(objective='reg:squarederror', random_state=42, colsample_bytree=0.9, learning_rate=0.1,\n",
    "                       max_depth=5, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.1,\n",
    "                       subsample=0.7, gamma=0.1)\n",
    "gb_model = GradientBoostingRegressor(subsample=0.9, min_samples_split=10, n_estimators=450, learning_rate=0.1,\n",
    "                                     min_samples_leaf=2, max_depth=5, random_state=42, loss='squared_error',\n",
    "                                     ccp_alpha=0.0, alpha=0.1, max_features='sqrt')\n",
    "\n",
    "# Crear el ensemble\n",
    "voting_regressor = VotingRegressor(estimators=[('rf', random_forest), ('xgb', xgb_reg), ('gb', gb_model)])\n",
    "\n",
    "# Ajustar el ensemble al conjunto de entrenamiento\n",
    "voting_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Entrenar los modelos base por separado\n",
    "random_forest.fit(X_train, y_train)\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Obtener la importancia de las características de los modelos base\n",
    "rf_importances = random_forest.feature_importances_\n",
    "xgb_importances = xgb_reg.feature_importances_\n",
    "gb_importances = gb_model.feature_importances_\n",
    "\n",
    "# Promediar las importancias\n",
    "average_importances = (rf_importances + xgb_importances + gb_importances) / 3\n",
    "\n",
    "# Crear un DataFrame con las características y sus importancias\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': average_importances\n",
    "})\n",
    "\n",
    "# Ordenar las características por importancia\n",
    "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Seleccionar las 50 características más importantes\n",
    "top_50_features = feature_importance_df['feature'][:50]\n",
    "\n",
    "# Filtrar X_train y X_test para quedarnos solo con las 50 características más importantes\n",
    "X_train_top50 = X_train[top_50_features]\n",
    "X_test_top50 = X_test[top_50_features]\n",
    "\n",
    "# Re-entrenar el modelo solo con las 50 características más importantes\n",
    "voting_regressor.fit(X_train_top50, y_train)\n",
    "\n",
    "# Realizar la validación cruzada\n",
    "cv = 3  # Número de folds\n",
    "kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "# Inicializamos listas para guardar las métricas de cada fold\n",
    "r2_scores = []\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "mape_scores = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_top50):\n",
    "    X_train_fold, X_val_fold = X_train_top50.iloc[train_index], X_train_top50.iloc[val_index]  # Usar .iloc con DataFrame\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]  # Usar .iloc con Series\n",
    "    \n",
    "    # Entrenar el modelo en el fold\n",
    "    voting_regressor.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predecir en el conjunto de validación\n",
    "    y_val_pred_log = voting_regressor.predict(X_val_fold)\n",
    "    \n",
    "    # Revertir la transformación logarítmica\n",
    "    y_val_pred = np.expm1(y_val_pred_log)\n",
    "    y_val_actual = np.expm1(y_val_fold)\n",
    "    \n",
    "    # Calcular las métricas para este fold\n",
    "    r2 = r2_score(y_val_actual, y_val_pred)\n",
    "    mse = mean_squared_error(y_val_actual, y_val_pred)\n",
    "    mae = mean_absolute_error(y_val_actual, y_val_pred)\n",
    "    mape = np.mean(np.abs((y_val_actual - y_val_pred) / y_val_actual)) * 100\n",
    "\n",
    "    # Guardar las métricas en las listas\n",
    "    r2_scores.append(r2)\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    mape_scores.append(mape)\n",
    "\n",
    "# Promedios de métricas en CV\n",
    "mean_r2_cv = np.mean(r2_scores)\n",
    "mean_mse_cv = np.mean(mse_scores)\n",
    "mean_rmse_cv = np.sqrt(mean_mse_cv)\n",
    "mean_mae_cv = np.mean(mae_scores)\n",
    "mean_mape_cv = np.mean(mape_scores)\n",
    "\n",
    "# Métricas de Cross-Validation\n",
    "print(\"=== Métricas de Cross-Validation ===\")\n",
    "print(f\"R² (promedio CV): {mean_r2_cv:.4f}\")\n",
    "print(f\"MSE (promedio CV): {mean_mse_cv:.4f}\")\n",
    "print(f\"RMSE (promedio CV): {mean_rmse_cv:.4f}\")\n",
    "print(f\"MAE (promedio CV): {mean_mae_cv:.4f}\")\n",
    "print(f\"MAPE (promedio CV): {mean_mape_cv:.4f}%\")\n",
    "\n",
    "# Predicciones con el conjunto de test\n",
    "y_test_pred_log = voting_regressor.predict(X_test_top50)\n",
    "\n",
    "# Revertir la transformación logarítmica para las predicciones del test\n",
    "y_test_pred = np.expm1(y_test_pred_log)\n",
    "y_test_actual = np.expm1(y_test)\n",
    "\n",
    "# Calcular las métricas para el conjunto de test\n",
    "r2_test = r2_score(y_test_actual, y_test_pred)\n",
    "mse_test = mean_squared_error(y_test_actual, y_test_pred)\n",
    "mae_test = mean_absolute_error(y_test_actual, y_test_pred)\n",
    "mape_test = np.mean(np.abs((y_test_actual - y_test_pred) / y_test_actual)) * 100\n",
    "\n",
    "# Imprimir métricas de test\n",
    "print(\"=== Métricas de Test ===\")\n",
    "print(f\"R² (Test): {r2_test:.4f}\")\n",
    "print(f\"MSE (Test): {mse_test:.4f}\")\n",
    "print(f\"RMSE (Test): {np.sqrt(mse_test):.4f}\")\n",
    "print(f\"MAE (Test): {mae_test:.4f}\")\n",
    "print(f\"MAPE (Test): {mape_test:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metricas con techo 120k y top 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Métricas de Cross-Validation ===\n",
      "R² (promedio CV): 0.5995\n",
      "MSE (promedio CV): 161627630.2319\n",
      "RMSE (promedio CV): 12713.2856\n",
      "MAE (promedio CV): 9232.5375\n",
      "MAPE (promedio CV): 19.5213%\n",
      "=== Métricas de Test ===\n",
      "R² (Test): 0.6114\n",
      "MSE (Test): 160014659.3944\n",
      "RMSE (Test): 12649.6901\n",
      "MAE (Test): 8904.9917\n",
      "MAPE (Test): 19.5945%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, VotingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Asegúrate de convertir los arrays de numpy a pandas DataFrame y Series si es necesario\n",
    "X_train = pd.DataFrame(X_train)  # Si X_train es un ndarray, conviértelo en DataFrame\n",
    "y_train = pd.Series(y_train)  # Si y_train es un ndarray, conviértelo en Series\n",
    "X_test = pd.DataFrame(X_test)  # Lo mismo para el conjunto de test\n",
    "y_test = pd.Series(y_test)\n",
    "\n",
    "# Definir tus mejores modelos\n",
    "random_forest = RandomForestRegressor(max_depth=9, min_samples_leaf=1, min_samples_split=2, n_estimators=300)\n",
    "xgb_reg = XGBRegressor(objective='reg:squarederror', random_state=42, colsample_bytree=0.9, learning_rate=0.1,\n",
    "                       max_depth=5, min_child_weight=5, n_estimators=200, reg_alpha=0.1, reg_lambda=0.1,\n",
    "                       subsample=0.7, gamma=0.1)\n",
    "gb_model = GradientBoostingRegressor(subsample=0.9, min_samples_split=10, n_estimators=450, learning_rate=0.1,\n",
    "                                     min_samples_leaf=2, max_depth=5, random_state=42, loss='squared_error',\n",
    "                                     ccp_alpha=0.0, alpha=0.1, max_features='sqrt')\n",
    "\n",
    "# Crear el ensemble\n",
    "voting_regressor = VotingRegressor(estimators=[('rf', random_forest), ('xgb', xgb_reg), ('gb', gb_model)])\n",
    "\n",
    "# Ajustar el ensemble al conjunto de entrenamiento\n",
    "voting_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Entrenar los modelos base por separado\n",
    "random_forest.fit(X_train, y_train)\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Obtener la importancia de las características de los modelos base\n",
    "rf_importances = random_forest.feature_importances_\n",
    "xgb_importances = xgb_reg.feature_importances_\n",
    "gb_importances = gb_model.feature_importances_\n",
    "\n",
    "# Promediar las importancias\n",
    "average_importances = (rf_importances + xgb_importances + gb_importances) / 3\n",
    "\n",
    "# Crear un DataFrame con las características y sus importancias\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': average_importances\n",
    "})\n",
    "\n",
    "# Ordenar las características por importancia\n",
    "feature_importance_df = feature_importance_df.sort_values(by='importance', ascending=False)\n",
    "\n",
    "# Seleccionar las 50 características más importantes\n",
    "top_50_features = feature_importance_df['feature'][:50]\n",
    "\n",
    "# Filtrar X_train y X_test para quedarnos solo con las 50 características más importantes\n",
    "X_train_top50 = X_train[top_50_features]\n",
    "X_test_top50 = X_test[top_50_features]\n",
    "\n",
    "# Re-entrenar el modelo solo con las 50 características más importantes\n",
    "voting_regressor.fit(X_train_top50, y_train)\n",
    "\n",
    "# Realizar la validación cruzada\n",
    "cv = 3  # Número de folds\n",
    "kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "\n",
    "# Inicializamos listas para guardar las métricas de cada fold\n",
    "r2_scores = []\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "mape_scores = []\n",
    "\n",
    "for train_index, val_index in kf.split(X_train_top50):\n",
    "    X_train_fold, X_val_fold = X_train_top50.iloc[train_index], X_train_top50.iloc[val_index]  # Usar .iloc con DataFrame\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]  # Usar .iloc con Series\n",
    "    \n",
    "    # Entrenar el modelo en el fold\n",
    "    voting_regressor.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predecir en el conjunto de validación\n",
    "    y_val_pred_log = voting_regressor.predict(X_val_fold)\n",
    "    \n",
    "    # Revertir la transformación logarítmica\n",
    "    y_val_pred = np.expm1(y_val_pred_log)\n",
    "    y_val_actual = np.expm1(y_val_fold)\n",
    "    \n",
    "    # Calcular las métricas para este fold\n",
    "    r2 = r2_score(y_val_actual, y_val_pred)\n",
    "    mse = mean_squared_error(y_val_actual, y_val_pred)\n",
    "    mae = mean_absolute_error(y_val_actual, y_val_pred)\n",
    "    mape = np.mean(np.abs((y_val_actual - y_val_pred) / y_val_actual)) * 100\n",
    "\n",
    "    # Guardar las métricas en las listas\n",
    "    r2_scores.append(r2)\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    mape_scores.append(mape)\n",
    "\n",
    "# Promedios de métricas en CV\n",
    "mean_r2_cv = np.mean(r2_scores)\n",
    "mean_mse_cv = np.mean(mse_scores)\n",
    "mean_rmse_cv = np.sqrt(mean_mse_cv)\n",
    "mean_mae_cv = np.mean(mae_scores)\n",
    "mean_mape_cv = np.mean(mape_scores)\n",
    "\n",
    "# Métricas de Cross-Validation\n",
    "print(\"=== Métricas de Cross-Validation ===\")\n",
    "print(f\"R² (promedio CV): {mean_r2_cv:.4f}\")\n",
    "print(f\"MSE (promedio CV): {mean_mse_cv:.4f}\")\n",
    "print(f\"RMSE (promedio CV): {mean_rmse_cv:.4f}\")\n",
    "print(f\"MAE (promedio CV): {mean_mae_cv:.4f}\")\n",
    "print(f\"MAPE (promedio CV): {mean_mape_cv:.4f}%\")\n",
    "\n",
    "# Predicciones con el conjunto de test\n",
    "y_test_pred_log = voting_regressor.predict(X_test_top50)\n",
    "\n",
    "# Revertir la transformación logarítmica para las predicciones del test\n",
    "y_test_pred = np.expm1(y_test_pred_log)\n",
    "y_test_actual = np.expm1(y_test)\n",
    "\n",
    "# Calcular las métricas para el conjunto de test\n",
    "r2_test = r2_score(y_test_actual, y_test_pred)\n",
    "mse_test = mean_squared_error(y_test_actual, y_test_pred)\n",
    "mae_test = mean_absolute_error(y_test_actual, y_test_pred)\n",
    "mape_test = np.mean(np.abs((y_test_actual - y_test_pred) / y_test_actual)) * 100\n",
    "\n",
    "# Imprimir métricas de test\n",
    "print(\"=== Métricas de Test ===\")\n",
    "print(f\"R² (Test): {r2_test:.4f}\")\n",
    "print(f\"MSE (Test): {mse_test:.4f}\")\n",
    "print(f\"RMSE (Test): {np.sqrt(mse_test):.4f}\")\n",
    "print(f\"MAE (Test): {mae_test:.4f}\")\n",
    "print(f\"MAPE (Test): {mape_test:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proyecto_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
