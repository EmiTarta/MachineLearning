{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_val_score, cross_validate, cross_val_predict, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, make_scorer\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.decomposition import PCA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset con filtro de las features elegidas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del DataFrame original: (1934, 397)\n",
      "Tamaño del DataFrame filtrado: (1743, 397)\n"
     ]
    }
   ],
   "source": [
    "# Cargar el DataFrame desde el archivo\n",
    "with open('../Pickles/df_final.pickle', 'rb') as archivo:\n",
    "    df = pickle.load(archivo)\n",
    "# Establecer la opción para mostrar todas las columnas\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "nombres=df.drop(['CompTotal'], axis=1).columns\n",
    "\n",
    "limite_inferior = 18000\n",
    "limite_superior = 100000\n",
    "\n",
    "df_filtrado = df[(df['CompTotal'] >= limite_inferior) & (df['CompTotal'] <= limite_superior)]\n",
    "\n",
    "print(f\"Tamaño del DataFrame original: {df.shape}\")\n",
    "print(f\"Tamaño del DataFrame filtrado: {df_filtrado.shape}\")\n",
    "\n",
    "df = df_filtrado\n",
    "\n",
    "df['CompTotal'] = np.log1p(df['CompTotal'])\n",
    "y = df['CompTotal']\n",
    "df = df.drop(columns=['CompTotal'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df[['MainBranch','EdLevel','YearsCode','YearsCodePro','AISelect', 'AISent',  'Frequency_1',\n",
    "        'Frequency_2','Frequency_3', 'Age_Grouped_Adulto','Age_Grouped_Joven','Age_Grouped_Senior', \n",
    "        'is_full_time', 'is_part_time', 'is_independent', 'num_jobs', 'is_other_employment', \n",
    "        'CodingActivities_encoded', 'LearnCode_encoded', 'LearnCodeOnline_encoded', \n",
    "        'DevType_encoded', 'Android-based', 'Linux-based', 'Windows-based',\n",
    "        'Industry_Category_Industria y Energía','Industry_Category_Otros Servicios',\n",
    "        'Industry_Category_Salud y Educación','Industry_Category_Servicios Financieros',\n",
    "        'Industry_Category_Tecnología y Servicios Digitales', 'BuyNewTool_Ask a generative AI tool',\n",
    "        'BuyNewTool_Ask developers I know/work with','BuyNewTool_Other','BuyNewTool_Other (please specify):',\n",
    "        'BuyNewTool_Read ratings or reviews on third party sites like G2 Crowd',\n",
    "        'BuyNewTool_Research companies that have advertised on sites I visit',\n",
    "        'BuyNewTool_Research companies that have emailed me','BuyNewTool_Start a free trial',\n",
    "        'BuyNewTool_Visit developer communities like Stack Overflow', 'LanguageHaveWorkedWith_', \n",
    "        'LanguageHaveWorkedWith_Ada', 'LanguageHaveWorkedWith_Apex', 'LanguageHaveWorkedWith_Assembly',\n",
    "        'LanguageHaveWorkedWith_Bash/Shell (all shells)', 'LanguageHaveWorkedWith_C', \n",
    "        'LanguageHaveWorkedWith_C#','LanguageHaveWorkedWith_C++', 'LanguageHaveWorkedWith_Clojure', \n",
    "        'LanguageHaveWorkedWith_Cobol','LanguageHaveWorkedWith_Crystal','LanguageHaveWorkedWith_Dart',\n",
    "        'LanguageHaveWorkedWith_Delphi','LanguageHaveWorkedWith_Elixir','LanguageHaveWorkedWith_Erlang',\n",
    "        'LanguageHaveWorkedWith_F#','LanguageHaveWorkedWith_Fortran','LanguageHaveWorkedWith_GDScript',\n",
    "        'LanguageHaveWorkedWith_Go','LanguageHaveWorkedWith_Groovy','LanguageHaveWorkedWith_HTML/CSS',\n",
    "        'LanguageHaveWorkedWith_Haskell','LanguageHaveWorkedWith_Java','LanguageHaveWorkedWith_JavaScript',\n",
    "        'LanguageHaveWorkedWith_Julia','LanguageHaveWorkedWith_Kotlin','LanguageHaveWorkedWith_Lisp',\n",
    "        'LanguageHaveWorkedWith_Lua', 'LanguageHaveWorkedWith_MATLAB','LanguageHaveWorkedWith_Nim', \n",
    "        'LanguageHaveWorkedWith_OCaml','LanguageHaveWorkedWith_Objective-C','LanguageHaveWorkedWith_PHP', \n",
    "        'LanguageHaveWorkedWith_Perl', 'LanguageHaveWorkedWith_PowerShell', 'LanguageHaveWorkedWith_Prolog',\n",
    "        'LanguageHaveWorkedWith_Python', 'LanguageHaveWorkedWith_R', 'LanguageHaveWorkedWith_Ruby', \n",
    "        'LanguageHaveWorkedWith_Rust', 'LanguageHaveWorkedWith_SQL', 'LanguageHaveWorkedWith_Scala', \n",
    "        'LanguageHaveWorkedWith_Solidity', 'LanguageHaveWorkedWith_Swift', 'LanguageHaveWorkedWith_TypeScript', \n",
    "        'LanguageHaveWorkedWith_VBA', 'LanguageHaveWorkedWith_Visual Basic (.Net)', \n",
    "        'LanguageHaveWorkedWith_Zig', 'LanguageWantToWorkWith_', 'LanguageWantToWorkWith_Ada', \n",
    "        'LanguageWantToWorkWith_Apex', 'LanguageWantToWorkWith_Assembly', 'LanguageWantToWorkWith_Bash/Shell (all shells)',\n",
    "        'LanguageWantToWorkWith_C', 'LanguageWantToWorkWith_C#', 'LanguageWantToWorkWith_C++', \n",
    "        'LanguageWantToWorkWith_Clojure', 'LanguageWantToWorkWith_Crystal', 'LanguageWantToWorkWith_Dart', \n",
    "        'LanguageWantToWorkWith_Delphi', 'LanguageWantToWorkWith_Elixir', 'LanguageWantToWorkWith_Erlang', \n",
    "        'LanguageWantToWorkWith_F#', 'LanguageWantToWorkWith_Fortran', 'LanguageWantToWorkWith_GDScript', \n",
    "        'LanguageWantToWorkWith_Go', 'LanguageWantToWorkWith_Groovy', 'LanguageWantToWorkWith_HTML/CSS',\n",
    "        'LanguageWantToWorkWith_Haskell', 'LanguageWantToWorkWith_Java', 'LanguageWantToWorkWith_JavaScript', \n",
    "        'LanguageWantToWorkWith_Julia', 'LanguageWantToWorkWith_Kotlin', 'LanguageWantToWorkWith_Lisp', \n",
    "        'LanguageWantToWorkWith_Lua', 'LanguageWantToWorkWith_MATLAB', 'LanguageWantToWorkWith_Nim', \n",
    "        'LanguageWantToWorkWith_OCaml', 'LanguageWantToWorkWith_Objective-C', 'LanguageWantToWorkWith_PHP', \n",
    "        'LanguageWantToWorkWith_Perl', 'LanguageWantToWorkWith_PowerShell', \n",
    "        'LanguageWantToWorkWith_Prolog', 'LanguageWantToWorkWith_Python','LanguageWantToWorkWith_R',\n",
    "        'LanguageWantToWorkWith_Ruby', 'LanguageWantToWorkWith_Rust', 'LanguageWantToWorkWith_SQL', \n",
    "        'LanguageWantToWorkWith_Scala','LanguageWantToWorkWith_Solidity', 'LanguageWantToWorkWith_Swift', \n",
    "        'LanguageWantToWorkWith_TypeScript', 'LanguageWantToWorkWith_VBA', \n",
    "        'LanguageWantToWorkWith_Visual Basic (.Net)','LanguageWantToWorkWith_Zig', \n",
    "        'DatabaseHaveWorkedWith_', 'DatabaseHaveWorkedWith_BigQuery', 'DatabaseHaveWorkedWith_Cassandra', \n",
    "        'DatabaseHaveWorkedWith_Clickhouse', 'DatabaseHaveWorkedWith_Cloud Firestore', \n",
    "        'DatabaseHaveWorkedWith_Cockroachdb', 'DatabaseHaveWorkedWith_Cosmos DB', \n",
    "        'DatabaseHaveWorkedWith_Couch DB', 'DatabaseHaveWorkedWith_Couchbase', \n",
    "        'DatabaseHaveWorkedWith_Datomic', 'DatabaseHaveWorkedWith_DuckDB', 'DatabaseHaveWorkedWith_Dynamodb', \n",
    "        'DatabaseHaveWorkedWith_Elasticsearch', 'DatabaseHaveWorkedWith_Firebase Realtime Database', \n",
    "        'DatabaseHaveWorkedWith_Firebird', 'DatabaseHaveWorkedWith_H2', 'DatabaseHaveWorkedWith_IBM DB2', \n",
    "        'DatabaseHaveWorkedWith_InfluxDB', 'DatabaseHaveWorkedWith_MariaDB' , 'DatabaseHaveWorkedWith_Microsoft Access', \n",
    "        'DatabaseHaveWorkedWith_Microsoft SQL Server', 'DatabaseHaveWorkedWith_MongoDB', 'DatabaseHaveWorkedWith_MySQL', \n",
    "        'DatabaseHaveWorkedWith_Neo4J', 'DatabaseHaveWorkedWith_Oracle', 'DatabaseHaveWorkedWith_PostgreSQL', \n",
    "        'DatabaseHaveWorkedWith_RavenDB', 'DatabaseHaveWorkedWith_Redis', 'DatabaseHaveWorkedWith_SQLite', \n",
    "        'DatabaseHaveWorkedWith_Snowflake', 'DatabaseHaveWorkedWith_Solr', 'DatabaseHaveWorkedWith_Supabase',\n",
    "        'ToolsTechHaveWorkedWith_', 'ToolsTechHaveWorkedWith_APT', 'ToolsTechHaveWorkedWith_Ansible', \n",
    "        'ToolsTechHaveWorkedWith_Ant', 'ToolsTechHaveWorkedWith_Bun', 'ToolsTechHaveWorkedWith_Chef', \n",
    "        'ToolsTechHaveWorkedWith_Chocolatey', 'ToolsTechHaveWorkedWith_Composer', 'ToolsTechHaveWorkedWith_Dagger', \n",
    "        'ToolsTechHaveWorkedWith_Docker', 'ToolsTechHaveWorkedWith_Godot','ToolsTechHaveWorkedWith_Google Test', \n",
    "        'ToolsTechHaveWorkedWith_Gradle', 'ToolsTechHaveWorkedWith_Homebrew', 'ToolsTechHaveWorkedWith_Kubernetes', \n",
    "        'ToolsTechHaveWorkedWith_MSBuild', 'ToolsTechHaveWorkedWith_Make', 'ToolsTechHaveWorkedWith_Maven (build tool)', \n",
    "        'ToolsTechHaveWorkedWith_Ninja', 'ToolsTechHaveWorkedWith_Nix', 'ToolsTechHaveWorkedWith_NuGet', 'ToolsTechHaveWorkedWith_Pacman', \n",
    "        'ToolsTechHaveWorkedWith_Pip', 'ToolsTechHaveWorkedWith_Podman', 'ToolsTechHaveWorkedWith_Pulumi', 'ToolsTechHaveWorkedWith_Puppet', \n",
    "        'ToolsTechHaveWorkedWith_Terraform', 'ToolsTechHaveWorkedWith_Unity 3D', 'ToolsTechHaveWorkedWith_Unreal Engine', \n",
    "        'ToolsTechHaveWorkedWith_Visual Studio Solution', 'ToolsTechHaveWorkedWith_Vite', 'ToolsTechHaveWorkedWith_Webpack',\n",
    "          'ToolsTechHaveWorkedWith_Yarn', 'ToolsTechHaveWorkedWith_npm', 'ToolsTechHaveWorkedWith_pnpm']]\n",
    "# Dividir los datos en train y test\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, random_state=42)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Reducir las columnas a la mitad con PCA\n",
    "n_columns = X_train.shape[1]  # Número de columnas originales\n",
    "pca = PCA(n_components=n_columns // 2)  # Reducir a la mitad\n",
    "X_train_reduced = pca.fit_transform(X_train)\n",
    "X_test_reduced = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primer modelo: baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Métricas de Cross-Validation ===\n",
      "R² (promedio CV): 0.5941\n",
      "MSE (promedio CV): 211685540.0328\n",
      "RMSE (promedio CV): 14549.4172\n",
      "MAE (promedio CV): 9992.2734\n",
      "MAPE (promedio CV): 20.0289%\n"
     ]
    }
   ],
   "source": [
    "# Asegúrate de convertir los arrays de numpy a pandas DataFrame y Series si es necesario\n",
    "X_train = pd.DataFrame(X_train)  # Si X_train es un ndarray, conviértelo en DataFrame\n",
    "y_train = pd.Series(y_train)  # Si y_train es un ndarray, conviértelo en Series\n",
    "\n",
    "# Crear un modelo de Random Forest\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "cv = 5  # Número de folds\n",
    "\n",
    "# Inicializamos listas para guardar las métricas de cada fold\n",
    "r2_scores = []\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "mape_scores = []\n",
    "\n",
    "# Realizar la validación cruzada\n",
    "kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]  # Usar .iloc con DataFrame\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]  # Usar .iloc con Series\n",
    "    \n",
    "    # Entrenar el modelo en el fold\n",
    "    rf_model.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predecir en el conjunto de validación\n",
    "    y_val_pred_log = rf_model.predict(X_val_fold)\n",
    "    \n",
    "    # Revertir la transformación logarítmica\n",
    "    y_val_pred = np.expm1(y_val_pred_log)\n",
    "    y_val_actual = np.expm1(y_val_fold)\n",
    "    \n",
    "    # Calcular las métricas para este fold\n",
    "    r2 = r2_score(y_val_actual, y_val_pred)\n",
    "    mse = mean_squared_error(y_val_actual, y_val_pred)\n",
    "    mae = mean_absolute_error(y_val_actual, y_val_pred)\n",
    "    mape = np.mean(np.abs((y_val_actual - y_val_pred) / y_val_actual)) * 100  \n",
    "    \n",
    "    # Guardar las métricas en las listas\n",
    "    r2_scores.append(r2)\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    mape_scores.append(mape)  # Guardar MAPE\n",
    "\n",
    "# Promedios de métricas en CV\n",
    "mean_r2_cv = np.mean(r2_scores)\n",
    "mean_mse_cv = np.mean(mse_scores)\n",
    "mean_rmse_cv = np.sqrt(mean_mse_cv)\n",
    "mean_mae_cv = np.mean(mae_scores)\n",
    "mean_mape_cv = np.mean(mape_scores)  # Promedio de MAPE\n",
    "\n",
    "# Métricas de Cross-Validation\n",
    "print(\"=== Métricas de Cross-Validation ===\")\n",
    "print(f\"R² (promedio CV): {mean_r2_cv:.4f}\")\n",
    "print(f\"MSE (promedio CV): {mean_mse_cv:.4f}\")\n",
    "print(f\"RMSE (promedio CV): {mean_rmse_cv:.4f}\")\n",
    "print(f\"MAE (promedio CV): {mean_mae_cv:.4f}\")\n",
    "print(f\"MAPE (promedio CV): {mean_mape_cv:.4f}%\")  # Imprimir MAPE promedio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El Linear Regressor da bastante bien, pruebo regularizandolo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor valor de alpha: 1.0\n",
      "=== Métricas de Cross-Validation (Ridge) ===\n",
      "R² (promedio CV): 0.4101\n",
      "MSE (promedio CV): 235957091.5062\n",
      "RMSE (promedio CV): 15306.5036\n",
      "MAE (promedio CV): 11334.6724\n",
      "MAPE (promedio CV): 24.5779%\n"
     ]
    }
   ],
   "source": [
    "# Asegúrate de que X_train y y_train sean DataFrame y Series de pandas\n",
    "X_train = pd.DataFrame(X_train)\n",
    "y_train = pd.Series(y_train)\n",
    "\n",
    "# Definir el rango de valores de alpha para la búsqueda en cuadrícula\n",
    "param_grid = {'alpha': np.logspace(-6, 6, 13)}  # Valores de alpha entre 0.0001 y 10000\n",
    "\n",
    "# Inicialización del modelo Ridge\n",
    "ridge_model = Ridge(random_state=42)\n",
    "\n",
    "# Configuración de GridSearchCV para encontrar el mejor alpha\n",
    "grid_search = GridSearchCV(estimator=ridge_model, param_grid=param_grid, \n",
    "                           scoring='neg_mean_squared_error', cv=5, n_jobs=-1)\n",
    "\n",
    "# Ajustar el modelo a los datos de entrenamiento\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Imprimir el mejor valor de alpha\n",
    "print(f\"Mejor valor de alpha: {grid_search.best_params_['alpha']}\")\n",
    "\n",
    "# Mejor modelo después de GridSearchCV\n",
    "best_ridge_model = grid_search.best_estimator_\n",
    "\n",
    "# Realizar la validación cruzada\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Listas para almacenar las métricas de cada fold\n",
    "r2_scores = []\n",
    "mse_scores = []\n",
    "rmse_scores = []\n",
    "mae_scores = []\n",
    "mape_scores = []\n",
    "\n",
    "for train_index, val_index in cv.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Entrenar el mejor modelo Ridge en el fold actual\n",
    "    best_ridge_model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "    # Hacer predicciones en el fold de validación (en la escala logarítmica)\n",
    "    y_val_pred_log = best_ridge_model.predict(X_val_fold)\n",
    "\n",
    "    # Aplicar la transformación inversa a las predicciones y a los valores reales\n",
    "    y_val_pred = np.expm1(y_val_pred_log)\n",
    "    y_val_original = np.expm1(y_val_fold)\n",
    "\n",
    "    # Calcular métricas en la escala original\n",
    "    r2_scores.append(r2_score(y_val_original, y_val_pred))\n",
    "    mse_scores.append(mean_squared_error(y_val_original, y_val_pred))\n",
    "    rmse_scores.append(np.sqrt(mse_scores[-1]))\n",
    "    mae_scores.append(mean_absolute_error(y_val_original, y_val_pred))\n",
    "    mape_scores.append(np.mean(np.abs((y_val_original - y_val_pred) / y_val_original)) * 100)\n",
    "\n",
    "# Calcular los promedios de las métricas\n",
    "mean_r2_cv = np.mean(r2_scores)\n",
    "mean_mse_cv = np.mean(mse_scores)\n",
    "mean_rmse_cv = np.mean(rmse_scores)\n",
    "mean_mae_cv = np.mean(mae_scores)\n",
    "mean_mape_cv = np.mean(mape_scores)\n",
    "\n",
    "# Imprimir las métricas\n",
    "print(\"=== Métricas de Cross-Validation (Ridge) ===\")\n",
    "print(f\"R² (promedio CV): {mean_r2_cv:.4f}\")\n",
    "print(f\"MSE (promedio CV): {mean_mse_cv:.4f}\")\n",
    "print(f\"RMSE (promedio CV): {mean_rmse_cv:.4f}\")\n",
    "print(f\"MAE (promedio CV): {mean_mae_cv:.4f}\")\n",
    "print(f\"MAPE (promedio CV): {mean_mape_cv:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No funciona el linear regressor, las relaciones entre los datos son mas complejas. Probamos con Modelos complejos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebo Gridsearch de Random Forest para mejorar los hiperparametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "Mejores parámetros: {'max_depth': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "=== Métricas de Cross-Validation ===\n",
      "R² (promedio CV): 0.5530\n",
      "MSE (promedio CV): 180254301.2373\n",
      "RMSE (promedio CV): 13425.8818\n",
      "MAE (promedio CV): 9468.7451\n",
      "MAPE (promedio CV): 20.0433%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "X_train = pd.DataFrame(X_train)\n",
    "y_train = pd.Series(y_train)\n",
    "\n",
    "# Definir MAPE como función\n",
    "def mape(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Crear el scorer\n",
    "mape_scorer = make_scorer(mape, greater_is_better=False)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [300],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [2],\n",
    "    'min_samples_leaf': [1, 2]\n",
    "}\n",
    "\n",
    "# Crear un modelo de Random Forest\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring={\n",
    "        'r2': 'r2',\n",
    "        'neg_mse': 'neg_mean_squared_error',\n",
    "        'neg_mae': 'neg_mean_absolute_error',\n",
    "        'neg_mape': mape_scorer  # Usar la función MAPE definida arriba\n",
    "    },\n",
    "    refit='neg_mape',  # Reajustar el mejor modelo basado en MAPE\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "# Inicializamos listas para guardar las métricas de cada fold\n",
    "r2_scores = []\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "mape_scores = []\n",
    "\n",
    "# Realizar la validación cruzada\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]  # Usar .iloc con DataFrame\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]  # Usar .iloc con Series\n",
    "    \n",
    "    # Entrenar el modelo en el fold\n",
    "    grid_search.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predecir en el conjunto de validación\n",
    "    y_val_pred_log = grid_search.predict(X_val_fold)\n",
    "    \n",
    "    # Revertir la transformación logarítmica\n",
    "    y_val_pred = np.expm1(y_val_pred_log)\n",
    "    y_val_actual = np.expm1(y_val_fold)\n",
    "    \n",
    "    # Calcular las métricas para este fold\n",
    "    r2 = r2_score(y_val_actual, y_val_pred)\n",
    "    mse = mean_squared_error(y_val_actual, y_val_pred)\n",
    "    mae = mean_absolute_error(y_val_actual, y_val_pred)\n",
    "    mape = np.mean(np.abs((y_val_actual - y_val_pred) / y_val_actual)) * 100\n",
    "\n",
    "    # Guardar las métricas en las listas\n",
    "    r2_scores.append(r2)\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    mape_scores.append(mape) \n",
    "# Obtener el mejor modelo\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtener los mejores parámetros\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Mejores parámetros:\", best_params)\n",
    "\n",
    "# Promedios de métricas en CV\n",
    "mean_r2_cv = np.mean(r2_scores)\n",
    "mean_mse_cv = np.mean(mse_scores)\n",
    "mean_rmse_cv = np.sqrt(mean_mse_cv)\n",
    "mean_mae_cv = np.mean(mae_scores)\n",
    "mean_mape_cv = np.mean(mape_scores) \n",
    "\n",
    "# Métricas de Cross-Validation\n",
    "print(\"=== Métricas de Cross-Validation ===\")\n",
    "print(f\"R² (promedio CV): {mean_r2_cv:.4f}\")\n",
    "print(f\"MSE (promedio CV): {mean_mse_cv:.4f}\")\n",
    "print(f\"RMSE (promedio CV): {mean_rmse_cv:.4f}\")\n",
    "print(f\"MAE (promedio CV): {mean_mae_cv:.4f}\")\n",
    "print(f\"MAPE (promedio CV): {mean_mape_cv:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebo con XGBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n",
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n",
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\numpy\\ma\\core.py:2820: RuntimeWarning: invalid value encountered in cast\n",
      "  _data = np.array(data, dtype=dtype, copy=copy,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros: {'colsample_bytree': 0.9, 'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 5, 'min_child_weight': 3, 'n_estimators': 200, 'subsample': 0.7}\n",
      "=== Métricas de Cross-Validation ===\n",
      "R² (promedio CV): 0.5821\n",
      "MSE (promedio CV): 217752793.6484\n",
      "RMSE (promedio CV): 14756.4492\n",
      "MAE (promedio CV): 10461.4388\n",
      "MAPE (promedio CV): 21.1169%\n"
     ]
    }
   ],
   "source": [
    "# Asegúrate de que X_train y y_train sean DataFrame y Series de pandas\n",
    "X_train = pd.DataFrame(X_train)  # Si X_train es un ndarray, conviértelo en DataFrame\n",
    "y_train = pd.Series(y_train)  # Si y_train es un ndarray, conviértelo en Series\n",
    "\n",
    "# Definir el grid de hiperparámetros para GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 300],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_child_weight': [3, 5],\n",
    "    'subsample': [0.7, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.9],\n",
    "    'gamma': [0.1, 0.3],\n",
    "}\n",
    "\n",
    "# Crear un modelo de Random Forest\n",
    "xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "# Configuración de GridSearchCV con validación cruzada\n",
    "grid_search = GridSearchCV(estimator=xgb_model, \n",
    "                           param_grid=param_grid, \n",
    "                           scoring='neg_mean_absolute_error', \n",
    "                           cv=3, n_jobs=-1, verbose=1)\n",
    "\n",
    "cv = 3  # Número de folds\n",
    "\n",
    "# Inicializamos listas para guardar las métricas de cada fold\n",
    "r2_scores = []\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "mape_scores = []\n",
    "\n",
    "# Realizar la validación cruzada\n",
    "kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]  # Usar .iloc con DataFrame\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]  # Usar .iloc con Series\n",
    "    \n",
    "    # Entrenar el modelo en el fold\n",
    "    grid_search.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predecir en el conjunto de validación\n",
    "    y_val_pred_log = grid_search.predict(X_val_fold)\n",
    "    \n",
    "    # Revertir la transformación logarítmica\n",
    "    y_val_pred = np.expm1(y_val_pred_log)\n",
    "    y_val_actual = np.expm1(y_val_fold)\n",
    "    \n",
    "    # Calcular las métricas para este fold\n",
    "    r2 = r2_score(y_val_actual, y_val_pred)\n",
    "    mse = mean_squared_error(y_val_actual, y_val_pred)\n",
    "    mae = mean_absolute_error(y_val_actual, y_val_pred)\n",
    "    mape = np.mean(np.abs((y_val_actual - y_val_pred) / y_val_actual)) * 100\n",
    "\n",
    "    # Guardar las métricas en las listas\n",
    "    r2_scores.append(r2)\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    mape_scores.append(mape) \n",
    "# Obtener el mejor modelo\n",
    "best_rf_model = grid_search.best_estimator_\n",
    "\n",
    "# Obtener los mejores parámetros\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Mejores parámetros:\", best_params)\n",
    "\n",
    "# Promedios de métricas en CV\n",
    "mean_r2_cv = np.mean(r2_scores)\n",
    "mean_mse_cv = np.mean(mse_scores)\n",
    "mean_rmse_cv = np.sqrt(mean_mse_cv)\n",
    "mean_mae_cv = np.mean(mae_scores)\n",
    "mean_mape_cv = np.mean(mape_scores) \n",
    "\n",
    "# Métricas de Cross-Validation\n",
    "print(\"=== Métricas de Cross-Validation ===\")\n",
    "print(f\"R² (promedio CV): {mean_r2_cv:.4f}\")\n",
    "print(f\"MSE (promedio CV): {mean_mse_cv:.4f}\")\n",
    "print(f\"RMSE (promedio CV): {mean_rmse_cv:.4f}\")\n",
    "print(f\"MAE (promedio CV): {mean_mae_cv:.4f}\")\n",
    "print(f\"MAPE (promedio CV): {mean_mape_cv:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pruebo con Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "108 fits failed out of a total of 300.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "40 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of GradientBoostingRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "68 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of GradientBoostingRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [-0.1778478  -0.19143082         nan         nan -0.08053836         nan\n",
      " -0.18463631         nan         nan -0.07436565 -0.16786074 -0.08016017\n",
      "         nan         nan         nan -0.18444234         nan         nan\n",
      " -0.10571708 -0.19143082 -0.10962139 -0.1076612          nan -0.17873794\n",
      " -0.19143082 -0.13171874 -0.15967561 -0.18518948 -0.08315946 -0.10614914\n",
      " -0.10552816 -0.0805318  -0.07922869 -0.15340865 -0.08175872         nan\n",
      " -0.07548989 -0.07609212 -0.08652973 -0.17514671 -0.19142628 -0.19142506\n",
      "         nan         nan -0.19144867         nan         nan         nan\n",
      "         nan         nan -0.19143082 -0.08457494         nan -0.17676103\n",
      "         nan         nan         nan         nan         nan -0.07646968\n",
      "         nan -0.19143082 -0.07799099 -0.19143123 -0.19143082 -0.18006795\n",
      " -0.15279077 -0.15313183 -0.12627966 -0.10998704 -0.07603561 -0.19143082\n",
      "         nan         nan         nan -0.19146224         nan -0.19150634\n",
      " -0.19144251 -0.17674244         nan -0.1229029          nan         nan\n",
      " -0.19139069 -0.08193246         nan -0.13198609 -0.07842894 -0.19142199\n",
      " -0.10459462 -0.10982829 -0.08016464 -0.18457263 -0.14373955 -0.08463882\n",
      "         nan         nan -0.1745025  -0.19143082]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "108 fits failed out of a total of 300.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "40 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of GradientBoostingRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "68 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of GradientBoostingRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [-0.17872532 -0.19091291         nan         nan -0.08449323         nan\n",
      " -0.18417731         nan         nan -0.07870881 -0.16836707 -0.08355248\n",
      "         nan         nan         nan -0.18442783         nan         nan\n",
      " -0.10732869 -0.19091291 -0.10703105 -0.11030493         nan -0.17940638\n",
      " -0.19091291 -0.13113902 -0.16038323 -0.18524063 -0.08323702 -0.10650031\n",
      " -0.10837962 -0.08945439 -0.08292383 -0.1546834  -0.08576057         nan\n",
      " -0.07661177 -0.0786876  -0.08844603 -0.17534408 -0.1909154  -0.19091319\n",
      "         nan         nan -0.19087732         nan         nan         nan\n",
      "         nan         nan -0.19091291 -0.08982675         nan -0.17671833\n",
      "         nan         nan         nan         nan         nan -0.07806839\n",
      "         nan -0.19091291 -0.0823358  -0.19091118 -0.19091291 -0.18027713\n",
      " -0.1537022  -0.15459491 -0.12840848 -0.10572393 -0.07806005 -0.19091291\n",
      "         nan         nan         nan -0.19090169         nan -0.19095466\n",
      " -0.19091506 -0.17674962         nan -0.12258573         nan         nan\n",
      " -0.19092175 -0.08101084         nan -0.13424817 -0.0822241  -0.19091347\n",
      " -0.10372699 -0.112629   -0.08224348 -0.18434527 -0.14673762 -0.08630468\n",
      "         nan         nan -0.17461491 -0.19091291]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "108 fits failed out of a total of 300.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "68 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of GradientBoostingRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "40 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of GradientBoostingRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [-0.17789699 -0.1920652          nan         nan -0.08356449         nan\n",
      " -0.18536068         nan         nan -0.07699884 -0.16622038 -0.08500832\n",
      "         nan         nan         nan -0.18519137         nan         nan\n",
      " -0.10418914 -0.1920652  -0.10242891 -0.10438729         nan -0.17855937\n",
      " -0.1920652  -0.12562324 -0.15962951 -0.18561656 -0.08019963 -0.10227658\n",
      " -0.10361758 -0.08751505 -0.08096139 -0.15301113 -0.07957548         nan\n",
      " -0.07765157 -0.07513687 -0.0856385  -0.17569941 -0.19206607 -0.19206643\n",
      "         nan         nan -0.19208821         nan         nan         nan\n",
      "         nan         nan -0.1920652  -0.08102722         nan -0.17716245\n",
      "         nan         nan         nan         nan         nan -0.07708419\n",
      "         nan -0.1920652  -0.07802413 -0.19206187 -0.1920652  -0.18055696\n",
      " -0.15294005 -0.1516608  -0.12465973 -0.10203215 -0.07584066 -0.1920652\n",
      "         nan         nan         nan -0.19203971         nan -0.19209005\n",
      " -0.19206422 -0.17710733         nan -0.11579493         nan         nan\n",
      " -0.1920719  -0.08025202         nan -0.12723733 -0.08013669 -0.19206581\n",
      " -0.10021061 -0.10677673 -0.0800571  -0.1850934  -0.1424296  -0.08410499\n",
      "         nan         nan -0.17437908 -0.1920652 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "108 fits failed out of a total of 300.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "73 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of GradientBoostingRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of GradientBoostingRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [-0.17727528 -0.18993245         nan         nan -0.08170668         nan\n",
      " -0.18340266         nan         nan -0.07707515 -0.16637436 -0.07995207\n",
      "         nan         nan         nan -0.18330467         nan         nan\n",
      " -0.10585672 -0.18993245 -0.10855398 -0.10867155         nan -0.17776566\n",
      " -0.18993245 -0.13044393 -0.15935708 -0.18436413 -0.07992935 -0.10748152\n",
      " -0.10614694 -0.0810485  -0.07870224 -0.15249391 -0.07993676         nan\n",
      " -0.07521588 -0.07451032 -0.08308602 -0.17422299 -0.18993155 -0.18993233\n",
      "         nan         nan -0.18994208         nan         nan         nan\n",
      "         nan         nan -0.18993245 -0.08008185         nan -0.17592884\n",
      "         nan         nan         nan         nan         nan -0.07705261\n",
      "         nan -0.18993245 -0.07906382 -0.18993244 -0.18993245 -0.17902758\n",
      " -0.1525439  -0.15284698 -0.12738595 -0.10616565 -0.07486447 -0.18993245\n",
      "         nan         nan         nan -0.18993962         nan -0.18992119\n",
      " -0.18993044 -0.17568522         nan -0.12131596         nan         nan\n",
      " -0.18992834 -0.0794028          nan -0.13223219 -0.07740461 -0.18993139\n",
      " -0.10666678 -0.11081652 -0.08064546 -0.18323347 -0.1438167  -0.08540852\n",
      "         nan         nan -0.17340208 -0.18993245]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:540: FitFailedWarning: \n",
      "108 fits failed out of a total of 300.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "44 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of GradientBoostingRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "64 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 888, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\base.py\", line 1466, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\base.py\", line 666, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of GradientBoostingRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1102: UserWarning: One or more of the test scores are non-finite: [-0.17923249 -0.19277675         nan         nan -0.08866305         nan\n",
      " -0.18605194         nan         nan -0.07765865 -0.16838167 -0.08707353\n",
      "         nan         nan         nan -0.18604523         nan         nan\n",
      " -0.10711411 -0.19277675 -0.10908603 -0.11206882         nan -0.17990283\n",
      " -0.19277675 -0.13268278 -0.16134605 -0.18650919 -0.0836202  -0.10629047\n",
      " -0.10700008 -0.08460292 -0.08250024 -0.15473033 -0.08312321         nan\n",
      " -0.08107705 -0.07644648 -0.09105635 -0.17670177 -0.19277623 -0.19277676\n",
      "         nan         nan -0.192872           nan         nan         nan\n",
      "         nan         nan -0.19277675 -0.08767549         nan -0.17817876\n",
      "         nan         nan         nan         nan         nan -0.07766372\n",
      "         nan -0.19277675 -0.08316093 -0.19277616 -0.19277675 -0.18179259\n",
      " -0.1543428  -0.15522788 -0.12821145 -0.10691714 -0.07754533 -0.19277675\n",
      "         nan         nan         nan -0.19274843         nan -0.1927194\n",
      " -0.19278167 -0.17814121         nan -0.12367305         nan         nan\n",
      " -0.19279492 -0.08317354         nan -0.13317054 -0.08059643 -0.19277997\n",
      " -0.1058878  -0.11042427 -0.08208501 -0.18604046 -0.14511097 -0.08752248\n",
      "         nan         nan -0.17583164 -0.19277675]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores parámetros: {'subsample': 0.9, 'n_estimators': 450, 'min_samples_split': 10, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 5, 'learning_rate': 0.1, 'ccp_alpha': 0.0, 'alpha': 0.1}\n",
      "=== Métricas de Cross-Validation ===\n",
      "R² (promedio CV): 0.5819\n",
      "MSE (promedio CV): 218156677.6926\n",
      "RMSE (promedio CV): 14770.1279\n",
      "MAE (promedio CV): 10611.1752\n",
      "MAPE (promedio CV): 21.4742%\n"
     ]
    }
   ],
   "source": [
    "# Asegúrate de convertir los arrays de numpy a pandas DataFrame y Series si es necesario\n",
    "X_train = pd.DataFrame(X_train)  # Si X_train es un ndarray, conviértelo en DataFrame\n",
    "y_train = pd.Series(y_train)  # Si y_train es un ndarray, conviértelo en Series\n",
    "\n",
    "# --- Definir el Espacio de Búsqueda de Hiperparámetros ---\n",
    "param_distributions = {\n",
    "    'n_estimators': [int(x) for x in np.linspace(start=50, stop=500, num=10)],\n",
    "    'learning_rate': [0.001, 0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'alpha': [0.1, 0.5, 0.9], #parametro de regularizacion L1\n",
    "    'ccp_alpha': [0.0, 0.01, 0.1] #parametro para la poda\n",
    "}\n",
    "# --- Instanciar el Modelo y RandomizedSearchCV ---\n",
    "gbr = GradientBoostingRegressor(random_state=42, loss='squared_error') \n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=gbr,  # Cambiar a GradientBoostingRegressor\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=100,  # Número de combinaciones a probar\n",
    "    cv=3,  # Número de folds en la validación cruzada\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,  # Usar todos los procesadores disponibles\n",
    "    scoring='neg_mean_squared_error'  # Puedes usar otra métrica si lo prefieres\n",
    ")\n",
    "cv = 5  # Número de folds\n",
    "\n",
    "# Inicializamos listas para guardar las métricas de cada fold\n",
    "r2_scores = []\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "mape_scores = []\n",
    "\n",
    "# Realizar la validación cruzada\n",
    "kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]  # Usar .iloc con DataFrame\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]  # Usar .iloc con Series\n",
    "    \n",
    "    # Entrenar el modelo en el fold\n",
    "    random_search.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predecir en el conjunto de validación\n",
    "    y_val_pred_log = random_search.predict(X_val_fold)\n",
    "    \n",
    "    # Revertir la transformación logarítmica\n",
    "    y_val_pred = np.expm1(y_val_pred_log)\n",
    "    y_val_actual = np.expm1(y_val_fold)\n",
    "    \n",
    "    # Calcular las métricas para este fold\n",
    "    r2 = r2_score(y_val_actual, y_val_pred)\n",
    "    mse = mean_squared_error(y_val_actual, y_val_pred)\n",
    "    mae = mean_absolute_error(y_val_actual, y_val_pred)\n",
    "    mape = np.mean(np.abs((y_val_actual - y_val_pred) / y_val_actual)) * 100\n",
    "\n",
    "    # Guardar las métricas en las listas\n",
    "    r2_scores.append(r2)\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    mape_scores.append(mape) \n",
    "# Obtener el mejor modelo\n",
    "best_rf_model = random_search.best_estimator_\n",
    "\n",
    "# Obtener los mejores parámetros\n",
    "best_params = random_search.best_params_\n",
    "print(\"Mejores parámetros:\", best_params)\n",
    "\n",
    "# Promedios de métricas en CV\n",
    "mean_r2_cv = np.mean(r2_scores)\n",
    "mean_mse_cv = np.mean(mse_scores)\n",
    "mean_rmse_cv = np.sqrt(mean_mse_cv)\n",
    "mean_mae_cv = np.mean(mae_scores)\n",
    "mean_mape_cv = np.mean(mape_scores) \n",
    "\n",
    "# Métricas de Cross-Validation\n",
    "print(\"=== Métricas de Cross-Validation ===\")\n",
    "print(f\"R² (promedio CV): {mean_r2_cv:.4f}\")\n",
    "print(f\"MSE (promedio CV): {mean_mse_cv:.4f}\")\n",
    "print(f\"RMSE (promedio CV): {mean_rmse_cv:.4f}\")\n",
    "print(f\"MAE (promedio CV): {mean_mae_cv:.4f}\")\n",
    "print(f\"MAPE (promedio CV): {mean_mape_cv:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciona mejor un linear regressor simple que cualquier otro modelo.\n",
    "\n",
    "Pruebo con SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m y_train_fold, y_val_fold \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39miloc[train_index], y_train\u001b[38;5;241m.\u001b[39miloc[val_index]  \u001b[38;5;66;03m# Usar .iloc con Series\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Entrenar el modelo en el fold\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m \u001b[43mrandom_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_fold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_fold\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Predecir en el conjunto de validación\u001b[39;00m\n\u001b[0;32m     46\u001b[0m y_val_pred_log \u001b[38;5;241m=\u001b[39m random_search\u001b[38;5;241m.\u001b[39mpredict(X_val_fold)\n",
      "File \u001b[1;32mc:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1018\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1013\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1014\u001b[0m     )\n\u001b[0;32m   1016\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1018\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1959\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1957\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1958\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1959\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1960\u001b[0m \u001b[43m        \u001b[49m\u001b[43mParameterSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1961\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_distributions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_iter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\n\u001b[0;32m   1962\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1963\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\model_selection\\_search.py:964\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    958\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    959\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    960\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    961\u001b[0m         )\n\u001b[0;32m    962\u001b[0m     )\n\u001b[1;32m--> 964\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    983\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    985\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    987\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\joblib\\parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[0;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\joblib\\parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\emita\\.conda\\envs\\proyecto_ml\\lib\\site-packages\\joblib\\parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Asegúrate de convertir los arrays de numpy a pandas DataFrame y Series si es necesario\n",
    "X_train = pd.DataFrame(X_train)  # Si X_train es un ndarray, conviértelo en DataFrame\n",
    "y_train = pd.Series(y_train)  # Si y_train es un ndarray, conviértelo en Series\n",
    "\n",
    "# Definir el espacio de búsqueda de hiperparámetros\n",
    "param_distributions = {\n",
    "    'C': np.logspace(-3, 3, 7),\n",
    "    'kernel': ['rbf', 'poly', 'sigmoid', 'linear'],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1, 10, 100, 'scale', 'auto'],\n",
    "    'epsilon': [0.01, 0.1, 0.2, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "# Instanciar el modelo SVR\n",
    "svr = SVR()\n",
    "\n",
    "# Configurar RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=svr,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=100,  # Aumenta este valor si tienes tiempo\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "cv = 3  # Número de folds\n",
    "\n",
    "# Inicializamos listas para guardar las métricas de cada fold\n",
    "r2_scores = []\n",
    "mse_scores = []\n",
    "mae_scores = []\n",
    "mape_scores = []\n",
    "\n",
    "# Realizar la validación cruzada\n",
    "kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]  # Usar .iloc con DataFrame\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]  # Usar .iloc con Series\n",
    "    \n",
    "    # Entrenar el modelo en el fold\n",
    "    random_search.fit(X_train_fold, y_train_fold)\n",
    "    \n",
    "    # Predecir en el conjunto de validación\n",
    "    y_val_pred_log = random_search.predict(X_val_fold)\n",
    "    \n",
    "    # Revertir la transformación logarítmica\n",
    "    y_val_pred = np.expm1(y_val_pred_log)\n",
    "    y_val_actual = np.expm1(y_val_fold)\n",
    "    \n",
    "    # Calcular las métricas para este fold\n",
    "    r2 = r2_score(y_val_actual, y_val_pred)\n",
    "    mse = mean_squared_error(y_val_actual, y_val_pred)\n",
    "    mae = mean_absolute_error(y_val_actual, y_val_pred)\n",
    "    mape = np.mean(np.abs((y_val_actual - y_val_pred) / y_val_actual)) * 100\n",
    "\n",
    "    # Guardar las métricas en las listas\n",
    "    r2_scores.append(r2)\n",
    "    mse_scores.append(mse)\n",
    "    mae_scores.append(mae)\n",
    "    mape_scores.append(mape) \n",
    "# Obtener el mejor modelo\n",
    "best_rf_model = random_search.best_estimator_\n",
    "\n",
    "# Obtener los mejores parámetros\n",
    "best_params = random_search.best_params_\n",
    "print(\"Mejores parámetros:\", best_params)\n",
    "\n",
    "# Promedios de métricas en CV\n",
    "mean_r2_cv = np.mean(r2_scores)\n",
    "mean_mse_cv = np.mean(mse_scores)\n",
    "mean_rmse_cv = np.sqrt(mean_mse_cv)\n",
    "mean_mae_cv = np.mean(mae_scores)\n",
    "mean_mape_cv = np.mean(mape_scores) \n",
    "\n",
    "# Métricas de Cross-Validation\n",
    "print(\"=== Métricas de Cross-Validation ===\")\n",
    "print(f\"R² (promedio CV): {mean_r2_cv:.4f}\")\n",
    "print(f\"MSE (promedio CV): {mean_mse_cv:.4f}\")\n",
    "print(f\"RMSE (promedio CV): {mean_rmse_cv:.4f}\")\n",
    "print(f\"MAE (promedio CV): {mean_mae_cv:.4f}\")\n",
    "print(f\"MAPE (promedio CV): {mean_mape_cv:.4f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proyecto_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
